{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preference-based Policy Iteration (PBPI) Algorithm \n",
    "### Application on the Inverted pendulum problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tested Parameter Configuration:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "configs = {'S': [200]\n",
    "          , 'Actions' : [3]\n",
    "          , 'Roll-outs': [100]\n",
    "          , 'Significance' : [0.025, 0.05, 0.1]\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T22:51:07.768166Z",
     "start_time": "2021-01-04T22:51:05.611641Z"
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "### importing the necessary packages ###\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import custom_cartpole  # custom cart-pole environment\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "from scipy.stats import rankdata as rd\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "\n",
    "import io\n",
    "import base64\n",
    "import itertools\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T22:51:07.778963Z",
     "start_time": "2021-01-04T22:51:07.770186Z"
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "########## helper functions ##########\n",
    "\n",
    "# generate a random action from a given environment\n",
    "def random_action(environment, seed=10):\n",
    "    \"\"\" return a random action from the given environment. \"\"\"\n",
    "    \n",
    "    # set env. seeds for reproducibility\n",
    "    #environment.action_space.np_random.seed(seed) \n",
    "    #environment.seed(seed) \n",
    "    \n",
    "    return environment.action_space.sample()\n",
    "\n",
    "\n",
    "# generate a list of initial states from a given environment\n",
    "def generate_init_states_S(seed\n",
    "                           , env = 'CustomCartPole-v0'\n",
    "                           , sample_size = 10 # how many states to include in the sample\n",
    "                          ):\n",
    "    \"\"\" this function returns a list of randomly generated initial states from a given environment. \"\"\"\n",
    "    \n",
    "    # set the random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # define how many initial states to generate altogether\n",
    "    n_states = np.random.randint(low=201, high=301) \n",
    "    \n",
    "    # define how many states to sample from the generated states\n",
    "    n_states_sample = np.random.randint(low=sample_size, high=sample_size+1) \n",
    "\n",
    "    # define a list to store the generated initial states\n",
    "    init_states_S = []\n",
    "\n",
    "    # create a given environment object\n",
    "    env = gym.make(env)\n",
    "    env.action_space.np_random.seed(seed) # set env. seeds for reproducibility\n",
    "    env.seed(seed) # set env. seeds for reproducibility\n",
    "    env.reset(init_state = np.array([0,0,0,0]))\n",
    "\n",
    "    # generate initial states\n",
    "    s_count = 0\n",
    "    while s_count < n_states:\n",
    "\n",
    "        # step through the environment by taking random actions\n",
    "        state, reward, done, info = env.step(env.action_space.sample())  \n",
    "            \n",
    "        # If terminates, reset the environment and continue to next step\n",
    "        #   (without appending the termination state to the list).\n",
    "        # Increment 'n_states' count by 7 since last 7 states from the termination state are removed\n",
    "        #  to avoid having states close to termination in the initial state list.\n",
    "        if done: \n",
    "            env.reset(init_state = np.array([0,0,0,0]))\n",
    "            n_states+=7\n",
    "            init_states_S = init_states_S[:-7]\n",
    "            continue\n",
    "            \n",
    "        # append the observed state to the initial state list\n",
    "        init_states_S.append(state)\n",
    "        \n",
    "        s_count +=1\n",
    "      \n",
    "    env.close()\n",
    "    \n",
    "    # remove any duplicate state values from the list\n",
    "    state_str_li = []\n",
    "    for state in init_states_S:\n",
    "        state_str_li.append(\"\".join([str(item[0]) for item in [item.reshape(-1) for item in state.flatten()]]))\n",
    "\n",
    "    uniq, uni_id = np.unique(state_str_li, return_index=True)\n",
    "    init_states_S = [init_states_S[j] for j in uni_id]\n",
    "    \n",
    "    # sample the required number of states (uniform random sampling)\n",
    "    sampled_states = random.sample(init_states_S, n_states_sample)\n",
    "            \n",
    "    return sampled_states #init_states_S\n",
    "    \n",
    "\n",
    "# partition the action space of a given environment \n",
    "def partition_action_space(env_name:'string'\n",
    "                           , n_actions:'int'):\n",
    "    \"\"\"function to partitions the action space of an environment into a given number of actions`\"\"\"\n",
    "    \n",
    "    # initialize environment\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # partition the action space to a given number of actions\n",
    "    part_act_space = np.linspace(env.action_space.low[0,0]\n",
    "                                 ,env.action_space.high[0,0],n_actions)\n",
    "    \n",
    "    return part_act_space  \n",
    "\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T22:51:07.790343Z",
     "start_time": "2021-01-04T22:51:07.780880Z"
    }
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "### preference generation process ###\n",
    "\n",
    "def evaluate_preference(starting_state # starting state of roll-outs\n",
    "                        , action_1     # first action to execute at the starting-state\n",
    "                        , action_2     # second action to execute at the starting state\n",
    "                        , policy_in    # policy to folow\n",
    "                        , environment_name = 'CustomCartPole-v0'   # name of the environment\n",
    "                        , discount_fac = 1        # discounting factor\n",
    "                        , n_rollouts = 20         # number of roll-outs to generate per action\n",
    "                        , max_rollout_len = 1500  # maximum length of a roll-out\n",
    "                        , label_ranker = False    # whether to use the label-ranking model or not\n",
    "                        , p_sig = 0.05            # p-value to use for t-test (to compare returns of roll-outs)\n",
    "                        , tracking = False\n",
    "                        ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "    \n",
    "        - Roll-outs are generated at each state in the initial state set by starting from the given input action \n",
    "          and following the given policy afterwards. \n",
    "        - Returns of the roll-outs are used to generate preferences for the input action pair.\n",
    "        - Generated preferences are returned to be create a training dataset to learn the LabelRanker model.    \n",
    "    \"\"\"\n",
    "    \n",
    "    # initializing variables\n",
    "    policy = policy_in          \n",
    "    n_rollouts = n_rollouts     \n",
    "    gamma = discount_fac    \n",
    "    s_init = starting_state\n",
    "    max_traj_len = max_rollout_len \n",
    "        \n",
    "    # we store the num. actions executed within the evaluation process (to measure complexity)\n",
    "    action_count = 0 \n",
    "        \n",
    "    # dictionary to store input action values\n",
    "    actions = { 'one' : action_1    \n",
    "              , 'two' : action_2}    \n",
    "\n",
    "    # dictionary to store rewards of roll-outs\n",
    "    r = { 'one' : [None]*n_rollouts \n",
    "        , 'two' : [None]*n_rollouts}  \n",
    "\n",
    "    # dictionary to store average discounted return for each action\n",
    "    avg_r = {}  \n",
    "    \n",
    "    # select each action of the input actions to generate roll-outs:\n",
    "    for action_key, action_value in actions.items():\n",
    "\n",
    "        # generate the defined number of roll-outs for selected action\n",
    "        for rollout in range(n_rollouts):\n",
    "\n",
    "            # create an environment object and set the starting state to the input (initial) state\n",
    "            env = gym.make(environment_name)\n",
    "            env.reset(init_state=s_init) # modified env.reset() in custom env: it accepts a starting state\n",
    "\n",
    "            # genereate random noice for action\n",
    "            rand_act_noice =  np.array([[np.random.uniform(low = -.2,high=.2)]])\n",
    "                                            \n",
    "            # apply the action (custom environment accepts float actions)\n",
    "            observation, reward, done, info = env.step(np.clip(action_value + rand_act_noice,-1,1)) # clip action value to (-1,1) range\n",
    "            \n",
    "            # define the history variable to store the last observed state\n",
    "            hist = observation \n",
    "            \n",
    "            # add the immediate reward received after executing the action\n",
    "            r[action_key][rollout] = reward  \n",
    "\n",
    "            # follow the given policy to generate a roll-out trajectory \n",
    "            traj_len = 1\n",
    "            while traj_len < max_traj_len and not done: \n",
    "                \n",
    "                # sample next state using the label-ranking model (if TRUE)\n",
    "                if label_ranker: \n",
    "                    observation, reward, done, info = env.step(policy.label_ranking_policy(hist))\n",
    "                    \n",
    "                    # replace current history with the observed state\n",
    "                    hist = observation\n",
    "                    action_count+=1\n",
    "                \n",
    "                # sample next state using a random policy\n",
    "                else: \n",
    "                    observation, reward, done, info = env.step(policy(env))\n",
    "                    action_count+=1\n",
    "\n",
    "                # compute discounted-reward at each step of the roll-out and store the roll-out return\n",
    "                r[action_key][rollout] += (gamma**traj_len) * reward\n",
    "\n",
    "                traj_len += 1\n",
    "\n",
    "            # close the environment after creating roll-outs\n",
    "            env.close()\n",
    "            del env\n",
    "        \n",
    "        # calculate the average discounted returns of the two actions\n",
    "        avg_r[action_key]  = sum(r[action_key]) / len(r[action_key])\n",
    "\n",
    "    # run a t-test to check whether the observed difference between average returns is significant\n",
    "    # (unpaird t-tests: equal variance)\n",
    "    t_val, p_val = stats.ttest_ind(r['one'],r['two']) \n",
    "    \n",
    "    # track output\n",
    "    if tracking:\n",
    "        print(f\"state: {[state_dim.reshape(-1)[0] for state_dim in [s_init[2],s_init[3][0][0]]]} | a_j(R): {avg_r['one']} | a_k(R): {avg_r['two']} | sig: {'Yes' if (p_val <= p_sig) else '--'}\")\n",
    "    \n",
    "    # return preference information\n",
    "    if (avg_r['one'] > avg_r['two']) and (p_val <= p_sig):\n",
    "        return {'state': s_init\n",
    "               , 'a_j' : actions['one']\n",
    "               , 'a_k' : actions['two']\n",
    "               , 'preference_label' : 1}, action_count\n",
    "    \n",
    "    elif(avg_r['one'] < avg_r['two']) and (p_val <= p_sig):\n",
    "        return {'state': s_init\n",
    "               , 'a_j' : actions['one']\n",
    "               , 'a_k' : actions['two']\n",
    "               , 'preference_label' : 0}, action_count\n",
    "    \n",
    "    # return NaN if avg. returns are not significantly different from each other OR are equal\n",
    "    else: \n",
    "        return {'state': np.nan\n",
    "               , 'a_j' : np.nan\n",
    "               , 'a_k' : np.nan\n",
    "               , 'preference_label' : np.nan}, action_count\n",
    "    \n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T22:51:07.808150Z",
     "start_time": "2021-01-04T22:51:07.791792Z"
    }
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "### LabelRanker Model training process ###\n",
    "\n",
    "def train_model(train_data                  # collection of all preference data\n",
    "                , action_space              # action space of the task\n",
    "                , model_name:str            # name for the model (to store)\n",
    "                , batch_s = 4               # batch size to train the NN model\n",
    "                , mod_layers = [10]         # model configuration\n",
    "                , n_epochs = 1000           # num. of epochs to train the model\n",
    "                , l_rate = 0.01             # learning rate for the optimization process  \n",
    "                , show_train_plot = False   # flag to display the 'training-loss vs. epoch' plot\n",
    "                , show_dataset = False):    # flag to display the training dataset\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "    \n",
    "        - This function process all preference data to construct a training dataset for the LabelRanker model.\n",
    "        - One training sample takes the form:\n",
    "            X: [state-value (2-D)]\n",
    "            Y: [(normalized) ranking of actions (n-D)], where 'n' is the number of actions in the action space.\n",
    "        - For a given (2-D) state input, the (trained) model, i.e., LabelRanker, predicts the rank of \n",
    "           all possible actions at the input state \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    ### creating the training dataset ###\n",
    "        \n",
    "    # convert training data input to a dataframe | \n",
    "    # remove the rows that have NaN, i.e.,preference evaluations without any action preference\n",
    "    train_df = pd.DataFrame(train_data).dropna()\n",
    "\n",
    "    # create a key for each state in the dataset\n",
    "    # (only select the 'pendulum-velocity & pendulum-angle)\n",
    "    #train_df.loc[:, 'state_key'] = train_df.state.apply(lambda x: x[2].astype(str)+\"_\"+x[3].astype(str))\n",
    "    #train_df.loc[:, 'state_key'] = train_df.state.apply(lambda x: round(x[2].reshape(-1)[0],6).astype(str)+\"_\"+round(x[3].reshape(-1)[0],6).astype(str))\n",
    "    train_df.loc[:, 'state_key'] = train_df.state.apply(lambda x: x[2].reshape(-1)[0].astype(str)+\"_\"+x[3].reshape(-1)[0].astype(str))\n",
    "\n",
    "    \n",
    "    # ******************************** # EXPERIMENTAL STEP START\n",
    "    # create a full state key (state+action preference)\n",
    "    #train_df.loc[:, 'state_action_key'] = train_df.state.apply(lambda x: round(x[2],6).astype(str)+\"_\"+round(x[3],6).astype(str)) +\"_\"+ train_df.a_j.apply(lambda x: x[0][0].astype(str))+\"_\"+ train_df.a_k.apply(lambda x: x[0][0].astype(str)) \n",
    "\n",
    "    \n",
    "    # drop duplicates (if one training-set maintained) : only keep the first learned preference\n",
    "    #train_df.drop_duplicates(subset=['state_key'], keep='first', inplace=True)\n",
    "    #train_df.drop_duplicates(subset=['state_action_key'], keep='first', inplace=True)\n",
    "    \n",
    "    #train_df.drop(columns=['state_action_key'], inplace=True)\n",
    "    \n",
    "    # ******************************** # EXPERIMENTAL STEP END\n",
    "    \n",
    "    # check if the training dataset is empty \n",
    "    # (if empty, subsequent steps have to be skipped)\n",
    "    if not(train_df.shape[0]>0):\n",
    "        \n",
    "        # if training dataset is emtpy - return None (break the training loop)\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        ### computing action-preference counts for every action (for every states) ###\n",
    "        \n",
    "        # identify the 'prefered-action' at each 'state, action-pair' preference evaluation\n",
    "        train_df.loc[:,'prefered_action'] = train_df.apply(lambda row: row['a_j'][0][0] if row['preference_label'] == 1 else row['a_k'][0][0]  ,axis=1)\n",
    "\n",
    "        # compute the number of times each action is prefered at each state\n",
    "        action_preference_counts = train_df.groupby('state_key').prefered_action.value_counts().unstack()\n",
    "        action_preference_counts.replace(np.nan,0,inplace=True) # if an action is not preferred at a state, set pref. count to '0'\n",
    "\n",
    "        # remove the column index names of the `action_preference_counts' summary table\n",
    "        action_preference_counts.columns.name = None\n",
    "\n",
    "        # find any action(s) that was not preferred at all sampled states \n",
    "        # - this is important because a ranking for every possible action\n",
    "        #   at each state needs to be included in the training (label) data\n",
    "        missed_actions = [action for action in action_space if action not in action_preference_counts.columns.tolist()]\n",
    "        missed_actions = np.array(missed_actions).astype(action_preference_counts.columns.dtype) # convert to the same data-type of remaining columns\n",
    "\n",
    "        # add any missing actions to the `action_preference_counts' table\n",
    "        if len(missed_actions)>0:\n",
    "\n",
    "            # add the missing action (with a preference count of zero)\n",
    "            for action in missed_actions:\n",
    "                action_preference_counts.loc[:,action] = 0\n",
    "\n",
    "            # sort the actions in the summary according to arrangement in action space (ascending order)\n",
    "            action_preference_counts = action_preference_counts.reindex(sorted(action_preference_counts.columns), axis=1)    \n",
    "\n",
    "        \n",
    "        # convert the action-preference-counts (of actions at each state) to a vector and add it as a new column\n",
    "        #  - data in this column is used to create training labels\n",
    "        action_preference_counts.loc[:, 'preference_label_vector'] = pd.DataFrame({'label_data': action_preference_counts.iloc[:,0:].values.tolist()}).values\n",
    "\n",
    "        # append the column having action-preference-counts vectors to the training dataset\n",
    "        train_df = train_df.merge(right = action_preference_counts.loc[:,['preference_label_vector']]\n",
    "                                  , right_index= True\n",
    "                                  , left_on = 'state_key'\n",
    "                                  , how = 'left')\n",
    "        \n",
    "\n",
    "        # create the reduced training dataset \n",
    "        # - drop unnecessary columns & duplicate rows (which have duplicate data for same states)\n",
    "        train_df_reduced = train_df.loc[:,['state', 'state_key', 'preference_label_vector']]\n",
    "        train_df_reduced.drop_duplicates(subset=['state_key'],inplace=True)\n",
    "        train_df_reduced.preference_label_vector = train_df_reduced.preference_label_vector.apply(lambda row: np.array(row).astype(np.float)) # convert all label vectors to float\n",
    "        \n",
    "        if show_dataset:\n",
    "            print(f'Training data samples: {train_df_reduced.shape[0]}')\n",
    "            print(train_df_reduced.loc[:,['state_key', 'preference_label_vector']])\n",
    "        \n",
    "        ### preparing the training dataset for the neural network (LabelRanker) model ###\n",
    "\n",
    "        # normalize the action-preference-counts vectors (label data for the model)\n",
    "        # - this step produces the rankings:\n",
    "        # - i.e., the action(s) with the highest preference count(s) will have the highest value(s)\n",
    "        # - after normalization\n",
    "        output_labels_temp = np.array(train_df_reduced.preference_label_vector.tolist())\n",
    "        row_sums = output_labels_temp.sum(axis=1)\n",
    "        output_labels_normalized = output_labels_temp / row_sums[:, np.newaxis]\n",
    "        output_labels = torch.from_numpy(output_labels_normalized) # convert to tensor\n",
    "\n",
    "        # generate the input state data tensors (feature data for the model)\n",
    "        # - this should only include pendulum-angle and pendulum-velocity\n",
    "        #input_states  = torch.from_numpy(np.array(train_df_reduced.state.apply(lambda x: [x[2].astype(float),x[3].astype(float)]).tolist())) # only select pole-position and pole-velocity\n",
    "        #input_states  = torch.from_numpy(np.array(train_df_reduced.state.apply(lambda x: [round(x[2].reshape(-1)[0],6).astype(float),round(x[3].reshape(-1)[0],6).astype(float)]).tolist())) # only select pole-position and pole-velocity\n",
    "        input_states  = torch.from_numpy(np.array(train_df_reduced.state.apply(lambda x: [x[2].reshape(-1)[0].astype(float),x[3].reshape(-1)[0].astype(float)]).tolist())) # only select pole-position and pole-velocity\n",
    "\n",
    "        \n",
    "        # create TensorDataset\n",
    "        train_ds = TensorDataset(input_states , output_labels)\n",
    "        \n",
    "        # define the batch size\n",
    "        batch_size = batch_s #train_df_reduced.shape[1]\n",
    "        \n",
    "        # define the data loader\n",
    "        train_dl = DataLoader(train_ds\n",
    "                              , batch_size\n",
    "                              , shuffle=True\n",
    "                              #, drop_last=True\n",
    "                             )\n",
    "        \n",
    "        \n",
    "    ### defining and training the neural network (LabelRanker) model ###        \n",
    "    \n",
    "    class Model(nn.Module):\n",
    "\n",
    "        def __init__(self, input_state_len, output_label_len, layers, p=0.3):\n",
    "\n",
    "            super(Model,self).__init__()\n",
    "\n",
    "            all_layers = []\n",
    "            input_size = input_state_len\n",
    "\n",
    "            # create layers\n",
    "            for layer_dim in layers:\n",
    "                all_layers.append(nn.Linear(input_size, layer_dim))\n",
    "                all_layers.append(nn.LeakyReLU(inplace=True))\n",
    "                #all_layers.append(nn.BatchNorm1d(layer_dim))\n",
    "                #all_layers.append(nn.Dropout(p))\n",
    "                input_size = layer_dim\n",
    "\n",
    "            all_layers.append(nn.Linear(layers[-1], output_label_len))\n",
    "\n",
    "            self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "        def forward(self, state_vec):\n",
    "            x = self.layers(state_vec)\n",
    "            return x\n",
    "\n",
    "        \n",
    "    # create a NN model instance\n",
    "    model = Model(input_states.shape[1], output_labels.shape[1], mod_layers)\n",
    "\n",
    "    # define optimizer and loss\n",
    "    opt = torch.optim.SGD(model.parameters(), lr = l_rate)\n",
    "    loss_fn = F.mse_loss\n",
    "\n",
    "    # list to store losses\n",
    "    aggregated_losses = []\n",
    "\n",
    "    # defining a function to train the model\n",
    "    def fit(num_epochs, model, loss_fn, opt):\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            for xb,yb in train_dl:\n",
    "\n",
    "                # Generate predictions\n",
    "                pred = model(xb.float())\n",
    "                loss = loss_fn(pred, yb.float())\n",
    "\n",
    "                # Perform gradient descent\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "\n",
    "            aggregated_losses.append(loss_fn(model(input_states.float()), output_labels.float()).detach().numpy())\n",
    "\n",
    "        #print('\\nTraining loss: ', loss_fn(model(input_states.float()), output_labels.float()).detach().numpy(),'\\n')\n",
    "        \n",
    "        # return training loss\n",
    "        return loss_fn(model(input_states.float()), output_labels.float()).detach().numpy()\n",
    "    \n",
    "\n",
    "    # train the model\n",
    "    epochs = n_epochs\n",
    "    loss_v = fit(epochs, model, loss_fn, opt)\n",
    "\n",
    "    # save the trained model\n",
    "    PATH = f\"./models/{model_name}_pbpi_model.pt\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    # plot the model loss\n",
    "    if show_train_plot:\n",
    "        plt.plot(range(epochs), aggregated_losses)\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.title(f'Training samples: {train_df_reduced.shape[0]} | Training loss: {np.round(loss_v,5)}\\n')\n",
    "        plt.show()\n",
    "\n",
    "    # set the model to evaluation mode and return it\n",
    "    return model.eval()\n",
    "\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T22:51:07.815654Z",
     "start_time": "2021-01-04T22:51:07.809589Z"
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "### Derived policy using LabelRanker ###\n",
    "\n",
    "class Policy():\n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "    \n",
    "        - This Policy object takes a given neural network (LabelRanker) model and uses it to define a policy for the agent to follow\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, action_space, model, probs):\n",
    "        self.action_space = action_space # action space of the current environment\n",
    "        self.model = model               # trained NN (LabelRanker) model\n",
    "        self.probs = probs               # list of probabilities for actions\n",
    "        \n",
    "    def label_ranking_policy(self,obs):\n",
    "        \"\"\" Produces an action for a given state based on the LabelRanker model prediction\n",
    "            Note: only the pendulum-angle and pendulum-velocity of the input state are considered when producing an action\n",
    "        \n",
    "            At each input state:\n",
    "                - Highest ranked action is selected with a prob. of 0.95\n",
    "                - Second highest ranked action is selected with a prob. of 0.04\n",
    "                - Any remaining actions are selected with an equal proabability of .01 \"\"\"\n",
    "\n",
    "\n",
    "        # only select the pendulum-velocity and angle from the input state vector\n",
    "        #state_obs = np.array([obs[2].reshape(-1)[0],obs[3].reshape(-1)[0]]) \n",
    "        #state_obs = np.array([round(obs[2].reshape(-1)[0],6),round(obs[3].reshape(-1)[0],6)]) # rounded input\n",
    "        state_obs = np.array([obs[2].reshape(-1)[0],obs[3].reshape(-1)[0]])\n",
    "        \n",
    "        #state_obs = state_obs.reshape(-1,state_obs.shape[0]) # reshape to be a 2D array\n",
    "        state_obs = torch.from_numpy(state_obs) # convert to a tensor\n",
    "\n",
    "        # make ranking predictions for all actions\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(state_obs.float()) \n",
    "\n",
    "        # rank the indexes of actions (from highest ranked/preferred action to lowest)\n",
    "        #ranked_action_idx = (-rd(preds.detach().numpy())).argsort()[:preds.shape[1]]\n",
    "        ranked_action_idx = (-rd(preds.detach().numpy())).argsort()\n",
    "\n",
    "        \n",
    "        ### return the selected action ###\n",
    "        \n",
    "        # if there are more than 2 actions\n",
    "        if len(self.action_space)>2:\n",
    "            \n",
    "            # compute the probabilities for the 3rd action onward\n",
    "            remain_probs = .00/len(ranked_action_idx[2:])\n",
    "            n_remain_actions = ranked_action_idx.shape[0]-2\n",
    "\n",
    "            # since we add random noise to action, policy becomes stochastic (even if we select the 1st ranked action always)\n",
    "            # select one of the remaining actions 1% time\n",
    "            action = np.random.choice(ranked_action_idx,1 , p=[self.probs[0], self.probs[1]] + list(np.repeat(remain_probs,n_remain_actions)))[0]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # if there are only 2 actions: select highest preferred actions 95% and 5% of the time\n",
    "            action = np.random.choice(ranked_action_idx,1 , p=[self.probs[0], self.probs[1]])[0]\n",
    "        \n",
    "        # when action space is partitioned, return the corresponding action\n",
    "        # - a uniform noise term is added to action signals to make all state transitions non-deterministic \n",
    "        # clip action value to (-1,1) range\n",
    "        return np.array([[np.clip(self.action_space[int(action)] + np.array(np.random.uniform(low = -.2,high=.2),dtype=float),-1,1)]])\n",
    "    \n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T22:51:07.822106Z",
     "start_time": "2021-01-04T22:51:07.816963Z"
    }
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "### Evaluating the learned policy ####\n",
    "\n",
    "\n",
    "def run_evaluations(policy               # input policy\n",
    "                    , state_list         # list of initial states\n",
    "                    , step_thresh = 1000    # step-count (threshold)\n",
    "                    , env_name = 'CustomCartPole-v0' # name of the environment\n",
    "                    , simulations_per_state = 100 # number of simulations to generate per state\n",
    "                   ):  \n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "    \n",
    "        - For every state in a given list of initial states, 100 simulations are generate and the percentage of\n",
    "           these simulations that exceeds a predefined step-count threadhold (trajectory length) is computed to measure \n",
    "           the performance of the given input policy.\"\"\"\n",
    "    \n",
    "\n",
    "    simu_per_state = simulations_per_state\n",
    "        \n",
    "    # create an environment instance\n",
    "    env_test = gym.make(env_name)\n",
    "    \n",
    "    # variable to record the sufficient policy count (across all simulations)\n",
    "    suf_policy_count = 0\n",
    "    \n",
    "    # variable to record episodic returns\n",
    "    ep_returns = []\n",
    "    max_return = 0\n",
    "    min_return = 2000\n",
    "    \n",
    "    # iterate over all states in the state list\n",
    "    for state in state_list:        \n",
    "        \n",
    "        # generate 100 simulations from each state\n",
    "        for _ in range(simu_per_state):\n",
    "            \n",
    "            # set the starting state and the current observation to the given state \n",
    "            env_test.reset(init_state=state)\n",
    "            obs = state\n",
    "        \n",
    "            # variable to store the return of an episode\n",
    "            return_ep = 0 \n",
    "\n",
    "            # execute 1001 steps in the environment\n",
    "            for _ in range(1001):\n",
    "                action = policy.label_ranking_policy(obs) # generate action from the policy\n",
    "                observation, reward, done, info = env_test.step(action) # execute action\n",
    "                obs = observation     # set history\n",
    "                return_ep += reward   # compute return\n",
    "                if done: break\n",
    "\n",
    "            env_test.close()\n",
    "\n",
    "            # append the return of the episode\n",
    "            ep_returns.append(return_ep)\n",
    "            \n",
    "            # update the max and min return variables\n",
    "            max_return = max(max_return,return_ep)\n",
    "            min_return = min(min_return,return_ep)\n",
    "            \n",
    "            # increment the sufficient policy count if return exceeds given threshold\n",
    "            # (note: at every step, 1 reward is produced in the environment)\n",
    "            if return_ep>=step_thresh:\n",
    "                suf_policy_count += 1\n",
    "    \n",
    "\n",
    "    # returns\n",
    "    # 1. % sufficient policy counts (total sufficient policies/ total # evaluation runs)\n",
    "    # 2. 'avg. episodic return'\n",
    "    # 3. maximum episodic return (across all evaluations)\n",
    "    # 4. minimum episodic return (across all evaluations)\n",
    "    \n",
    "    return (suf_policy_count/(len(state_list)*simu_per_state))*100, (sum(ep_returns)/(len(state_list)*simu_per_state)), max_return, min_return \n",
    "\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for a single hyper-parameter configuration (multiple-runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T22:51:08.281975Z",
     "start_time": "2021-01-04T22:51:08.266522Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluations_per_config(s_size \n",
    "                           , n_actions\n",
    "                           , max_n_rollouts\n",
    "                           , sig_lvl\n",
    "                           , runs_per_config = 10\n",
    "                           , off_policy_explr = False\n",
    "                           , env_name = 'CustomCartPole-v0'\n",
    "                           , print_run_eval_plot = False\n",
    "                           ):\n",
    "    \n",
    "    #########################\n",
    "    ### PARAMETER INPUTS ###\n",
    "\n",
    "    ## hyper-parameters ##\n",
    "\n",
    "    env_name = env_name\n",
    "\n",
    "    s_size = s_size             # initial state stample size\n",
    "    n_actions = n_actions       # number of actions in the action space\n",
    "    n_rollouts = max_n_rollouts # max. number of roll-outs to generate per action\n",
    "    sig_lvl = sig_lvl           # statistical significance for action-pair comparisons\n",
    "    runs_per_config = runs_per_config  # training runs for a single parameter configuration\n",
    "\n",
    "    # hyper-parameter configurations (string)\n",
    "    param_config_string = f'Samples: {s_size} | Actions: {n_actions} | Roll-outs: {n_rollouts} | Significance: {sig_lvl}'\n",
    "    \n",
    "    ## task settings ##\n",
    "\n",
    "    seed = 2                                  # set seed\n",
    "    max_iterr = 10                            # max. num. of policy iterations\n",
    "    off_policy_exploration = off_policy_explr # trigger to use off-policy exploration [MY MODIFICATION]\n",
    "    eval_simu_per_state = 100                 # number of evaluation runs from each initial starting state (evaluation)\n",
    "    \n",
    "    model_name = f'CartPole_{s_size}_{n_actions}_{n_rollouts}_{sig_lvl}'      # name for the saved LabelRanker model\n",
    "\n",
    "    ## flags/triggers ##\n",
    "\n",
    "    print_iterr = False                   # trigger to print progress bars of training iterations\n",
    "    print_states_cover = False            # trigger to print progress bars of visited states\n",
    "    print_rollouts = False                # trigger printing roll-out results\n",
    "    print_training_plot = False            # trigger printing the training loss of LabelRanker Model\n",
    "    print_eval_plot = True                # trigger printing the evaluation results\n",
    "\n",
    "    #########################\n",
    "\n",
    "    ### variable initialization ###\n",
    "\n",
    "    env = gym.make(env_name)   # create environment\n",
    "    sample_states = generate_init_states_S(seed = seed, env = env_name, sample_size = s_size)  # generate sample states\n",
    "    act_space = partition_action_space(env_name = env_name, n_actions = n_actions) # partition the action space\n",
    "    act_pairs = list(itertools.combinations(act_space,2)) # generate action-pairs from the partitioned action space\n",
    "\n",
    "    print(f'\\nCurrently evaluated configs:\\n '+  param_config_string, end='\\r')\n",
    "\n",
    "    # Initialize the LabelRanker model and epoch configs\n",
    "    # Note: these configs were decided after testing different settings; there can be better/different choices\n",
    "    if s_size < 49:\n",
    "        model_config = [50]\n",
    "        epch_config  = 1000\n",
    "    elif s_size >= 49 and s_size < 149:\n",
    "        model_config = [100]\n",
    "        epch_config  = 2000\n",
    "    else:\n",
    "        model_config  = [125]\n",
    "        epch_config   = 2000\n",
    "\n",
    "\n",
    "    # list to store results of the evaluation run\n",
    "    run_results = []\n",
    "\n",
    "    # generate evaluations for a single hyper-parameter configuration\n",
    "    for run in tqdm.notebook.tqdm(range(runs_per_config), desc=\"Runs\"):\n",
    "\n",
    "        ### place holders for evaluation metrics ###\n",
    "\n",
    "        agg_pct_suff_policies = [] # list to store the % of learned sufficient policies\n",
    "        action_count_li = []       # list to store the action counts in each training iteration\n",
    "\n",
    "\n",
    "        ### flags, triggers and adjustments ###\n",
    "\n",
    "        label_r_flag = False       # trigger to start using the trained LabelRanker model \n",
    "        policy = random_action     # set the initial policy to a random policy\n",
    "        max_iterr = max_iterr + 1  # since iteration count starts from '1', increment the max. iteration count by 1\n",
    "\n",
    "\n",
    "        ### training loop ###\n",
    "\n",
    "        iterr = 1\n",
    "        while iterr < max_iterr:\n",
    "\n",
    "            train_data = []      # place-holder to store training data\n",
    "            actions_in_iterr = 0 # variable to store the num. actions excuted in each training iteration\n",
    "\n",
    "            for state in sample_states: # generate roll-outs from each starting state\n",
    "\n",
    "                for action_pair in act_pairs: # generate roll-outs for each action pair\n",
    "\n",
    "                    # generate preference data & executed num. of actions in each action pair evaluation step\n",
    "                    preference_out, actions_per_pair = evaluate_preference(starting_state = state\n",
    "                                                                         , action_1       = np.array([[action_pair[0]]])\n",
    "                                                                         , action_2       = np.array([[action_pair[1]]])\n",
    "                                                                         , policy_in      = policy\n",
    "                                                                         , label_ranker   = label_r_flag\n",
    "                                                                         , n_rollouts     = n_rollouts\n",
    "                                                                         , p_sig          = sig_lvl\n",
    "                                                                         , tracking       = False\n",
    "                                                                          )   \n",
    "\n",
    "                    # append the generated preference data to the training data list\n",
    "                    if preference_out is not None:\n",
    "                        train_data.append(preference_out) \n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    # compute/update the tot. # actions executed in the training iteration\n",
    "                    actions_in_iterr += actions_per_pair  \n",
    "\n",
    "            # generate the training dataset and learn the LabelRanker model\n",
    "            model = train_model(train_data     = train_data\n",
    "                                , action_space = act_space\n",
    "                                , model_name   = model_name \n",
    "                                , mod_layers   = model_config\n",
    "                                , batch_s      = 4\n",
    "                                , n_epochs     = epch_config \n",
    "                                , l_rate       = 0.1\n",
    "                                , show_train_plot = False\n",
    "                                , show_dataset    = False\n",
    "                                )\n",
    "\n",
    "\n",
    "            # When no traiing data is found, the LabelRanker model will not be trained. \n",
    "            # Therefore, break the current training iteration and continue to the next \n",
    "            # (after updating the aggregated evaluation results)\n",
    "            if model is None:\n",
    "\n",
    "                print(f'No training data collected!')\n",
    "\n",
    "                # update the tot. # actions executed across all training iterations\n",
    "                if iterr>1:\n",
    "                    action_count_li.append(actions_in_iterr+action_count_li[iterr-2])\n",
    "                else:\n",
    "                    action_count_li.append(actions_in_iterr)\n",
    "\n",
    "                # Add '0' to the evaluation results\n",
    "                agg_pct_suff_policies.append(0) # pct. of sufficient policies in evaluations\n",
    "\n",
    "                iterr += 1\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Derive a new policy using the trained model\n",
    "            if off_policy_exploration:\n",
    "\n",
    "                # Generate separate 'target' and 'behaviour' policies\n",
    "                # Target policy to be used in evaluations, and behaviour policy to generate roll-outs (training data)\n",
    "                target_policy = Policy(act_space, model, [1.0, 0.0]) # always select the highest ranked action\n",
    "                exp_policy = Policy(act_space, model, [0.5, 0.5])    # select the first two highest ranked actions w/ same prob. \n",
    "\n",
    "            else:\n",
    "\n",
    "                # Set both 'target' and 'behaviour' policies to follow the optimal policy\n",
    "                # I.e., always select the highest ranked action\n",
    "                target_policy = Policy(act_space, model, [1.0, 0.0])\n",
    "                exp_policy = Policy(act_space, model, [1.0, 0.0])\n",
    "\n",
    "\n",
    "            # update the tot. # actions executed across all training iterations\n",
    "            if iterr>1:\n",
    "                action_count_li.append(actions_in_iterr+action_count_li[iterr-2])\n",
    "            else:\n",
    "                action_count_li.append(actions_in_iterr)\n",
    "\n",
    "\n",
    "            # evaluate the performance of the learned policy\n",
    "            pct_succ_policies, x, y, z = run_evaluations(target_policy\n",
    "                                                        , sample_states\n",
    "                                                        , simulations_per_state = eval_simu_per_state\n",
    "                                                        , step_thresh = 1000 # steps needed for a sufficient policy\n",
    "                                                       ) \n",
    "\n",
    "\n",
    "            # record evaluation results (across training iterations)\n",
    "            agg_pct_suff_policies.append(pct_succ_policies) # pct. of sufficient policies in evaluations\n",
    "\n",
    "\n",
    "            ### TERMINATION CONDITION ###\n",
    "\n",
    "            # If the current policy's performance (% of sufficient policies) is less than \n",
    "            #  half of the last policy's performance, TERMINATE the training process\n",
    "\n",
    "            if iterr>1:\n",
    "                prvs_policy_perf = agg_pct_suff_policies[-2]\n",
    "                curr_policy_perf = agg_pct_suff_policies[-1]\n",
    "\n",
    "                if prvs_policy_perf * (0.5) > curr_policy_perf:\n",
    "                    print(f'Policy performance decreased! Run-{run} terminated!')\n",
    "\n",
    "                    # remove the records from the worsen policy\n",
    "                    agg_pct_suff_policies = agg_pct_suff_policies[:-1]\n",
    "                    action_count_li = action_count_li[:-1]\n",
    "                    \n",
    "                    break\n",
    "\n",
    "                    \n",
    "            # Start using the trained LabelRanker model\n",
    "            # The first policy of the training process is always a random-policy\n",
    "            # From the second iteration onward, it uses the learned LabelRanker model\n",
    "            label_r_flag = True\n",
    "\n",
    "            if label_r_flag is False:\n",
    "                policy = random_action # set the random policy\n",
    "            else:\n",
    "                policy = exp_policy\n",
    "\n",
    "            iterr += 1\n",
    "\n",
    "        # plot evaluation results of the training run \n",
    "        if print_run_eval_plot: \n",
    "            plt.clf()\n",
    "            plt.cla()\n",
    "            plt.close()\n",
    "\n",
    "            fig, ax2 = plt.subplots(figsize =(6,4))\n",
    "            ax2.plot(action_count_li, agg_pct_suff_policies, 'm-.', label = 'success rate')\n",
    "            ax2.set_xlabel('# actions')\n",
    "            ax2.set_ylabel('Pct. of sufficient policies')\n",
    "            ax2.legend(loc='upper left')\n",
    "            plt.title(f'Evaluation Results | Run: {run+1}')\n",
    "\n",
    "            plt.savefig(f'./train_imgs/{model_name}_{run}.png') # save the evaluation image\n",
    "            plt.show() \n",
    "        \n",
    "        # store the evaluation results of the training run\n",
    "        run_results.append({'S': s_size\n",
    "                           , 'Actions' : n_actions\n",
    "                           , 'Roll-outs': n_rollouts\n",
    "                           , 'Significance' : sig_lvl\n",
    "                           , 'run': run\n",
    "                           , 'action_record': action_count_li\n",
    "                           , 'SR': agg_pct_suff_policies})\n",
    "\n",
    "        if print_iterr:\n",
    "            pbar.close()\n",
    "            \n",
    "    # output the recorded evaluation results for the hyper-parameter configuration\n",
    "    return run_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-04T22:51:16.078Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e662222e3740df94e63d5787e6bba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluations', max=3.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently evaluated configs:\n",
      " Samples: 200 | Actions: 3 | Roll-outs: 100 | Significance: 0.025\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39907613b0341beaf3ae646778391e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Runs', max=10.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy performance decreased! Run-0 terminated!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "configs = {'S': [200]\n",
    "          , 'Actions' : [3]\n",
    "          , 'Roll-outs': [100]\n",
    "          , 'Significance' : [0.025, 0.05, 0.1]\n",
    "          }\n",
    "\n",
    "agg_results = []\n",
    "\n",
    "eval_count = len(configs['S'])*len(configs['Actions'])*len(configs['Roll-outs'])*len(configs['Significance'])\n",
    "\n",
    "pbar_evals = tqdm.notebook.tqdm(total=eval_count, desc=\"Evaluations\", leave=False)\n",
    "\n",
    "for sample_size in configs['S']:\n",
    "        \n",
    "    for rollout_max in configs['Roll-outs']:\n",
    "\n",
    "        for sig_lvl in configs['Significance']:\n",
    "\n",
    "            run_results = evaluations_per_config(s_size = sample_size\n",
    "                                                , n_actions = configs['Actions'][0]\n",
    "                                                , max_n_rollouts = rollout_max\n",
    "                                                , sig_lvl = sig_lvl\n",
    "                                                , runs_per_config = 10\n",
    "                                                , off_policy_explr = False\n",
    "                                                , print_run_eval_plot = True)\n",
    "\n",
    "            agg_results.append(run_results)\n",
    "\n",
    "            pbar_evals.update(1)\n",
    "                \n",
    "pbar_evals.close()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-26T21:57:06.390005Z",
     "start_time": "2020-12-26T21:57:06.222383Z"
    }
   },
   "outputs": [],
   "source": [
    "results_dfs = []\n",
    "for result in agg_results:\n",
    "    results_dfs.append(pd.DataFrame(result))\n",
    "\n",
    "results_df = pd.concat(results_dfs)\n",
    "\n",
    "results_df.to_excel('original_experiment_results_para_config_6.xlsx',index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
