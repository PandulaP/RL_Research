{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on the initial state selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis: Initial state selection affects the performance of the PbPI algorithm\n",
    "\n",
    "#### Analysis method:\n",
    "\n",
    "- Design a two sets of state pools with equal number of states\n",
    "    - First, generate a large pool of states by letting the agent follow a random policy for 1000 steps; if an episode terminates while executing this number of actions, the environment is reset and the remaining actions are executed.\n",
    "    - Afterwards, derive three sets of initial states by following the below approach:\n",
    "        - From the initial pool of states, randomly sample 50 states to derive a initial state distribution of the pendulum angle skewed to right.\n",
    "        - From the initial pool of states, randomly sample 50 states to derive a initial state distribution of the pendulum angle skewed to left.\n",
    "        - From the initial pool of states, divide the captured pendulum angle (state) into 10 equidistant partitions and sample 4 states from each partition:\n",
    "            - When partitioning the state pendulum angle state values, I define a upper and lower bounds on the pendulum angle as -45 degrees  (-0.77 rads) and + 45 degrees (+0.77 rads).\n",
    "            - This is due to the assumption that once the pendulum exceeds these thresholds, the agent will not be able to balance the pendulum again regardless how large the magnitute of the force would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "### importing the necessary packages ###\n",
    "########################################\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import custom_cartpole  # custom cart-pole environment\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "from scipy.stats import rankdata as rd\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "\n",
    "import io\n",
    "import base64\n",
    "import itertools\n",
    "import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "########## helper functions ##########\n",
    "########################################\n",
    "\n",
    "\n",
    "# generate a random action from a given environment\n",
    "def random_action(environment, seed=10):\n",
    "    \"\"\" return a random action from the given environment. \"\"\"\n",
    "    \n",
    "    # set env. seeds for reproducibility\n",
    "    #environment.action_space.np_random.seed(seed) \n",
    "    #environment.seed(seed) \n",
    "    \n",
    "    return environment.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01495557, -0.03289365,  0.00216083,  0.04515281])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CustomCartPole-v0') \n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.1415927"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.16595599]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.np_random.seed(1)\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are only interested in pole-angle and pole-velocity.\n",
    "# They corresponds to the third and fourth values of the state space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of initial states from a given environment\n",
    "def generate_init_states_S(seed\n",
    "                           , env = 'CustomCartPole-v0'\n",
    "                           , sample_size = 10 # how many states to include in the sample\n",
    "                          ):\n",
    "    \"\"\" this function returns a list of randomly generated initial states from a given environment. \"\"\"\n",
    "    \n",
    "    # set the random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # define how many initial states to generate altogether\n",
    "    n_states = np.random.randint(low=201, high=301) \n",
    "    \n",
    "    # define how many states to sample from the generated states\n",
    "    n_states_sample = np.random.randint(low=sample_size, high=sample_size+1) \n",
    "\n",
    "    # define a list to store the generated initial states\n",
    "    init_states_S = []\n",
    "\n",
    "    # create a given environment object\n",
    "    env = gym.make(env)\n",
    "    env.action_space.np_random.seed(seed) # set env. seeds for reproducibility\n",
    "    env.seed(seed) # set env. seeds for reproducibility\n",
    "    env.reset(init_state = np.array([0,0,0,0]))\n",
    "\n",
    "    # generate initial states\n",
    "    s_count = 0\n",
    "    while s_count < n_states:\n",
    "\n",
    "        # step through the environment by taking random actions\n",
    "        state, reward, done, info = env.step(env.action_space.sample())  \n",
    "            \n",
    "        # If terminates, reset the environment and continue to next step\n",
    "        #   (without appending the termination state to the list).\n",
    "        # Increment 'n_states' count by 7 since last 7 states from the termination state are removed\n",
    "        #  to avoid having states close to termination in the initial state list.\n",
    "        if done: \n",
    "            env.reset(init_state = np.array([0,0,0,0]))\n",
    "            n_states+=7\n",
    "            init_states_S = init_states_S[:-7]\n",
    "            continue\n",
    "            \n",
    "        # append the observed state to the initial state list\n",
    "        init_states_S.append(state)\n",
    "        \n",
    "        s_count +=1\n",
    "      \n",
    "    env.close()\n",
    "    \n",
    "    # remove any duplicate state values from the list\n",
    "    state_str_li = []\n",
    "    for state in init_states_S:\n",
    "        state_str_li.append(\"\".join([str(item[0]) for item in [item.reshape(-1) for item in state.flatten()]]))\n",
    "\n",
    "    uniq, uni_id = np.unique(state_str_li, return_index=True)\n",
    "    init_states_S = [init_states_S[j] for j in uni_id]\n",
    "    \n",
    "    # sample the required number of states (uniform random sampling)\n",
    "    sampled_states = random.sample(init_states_S, n_states_sample)\n",
    "            \n",
    "    return sampled_states #init_states_S\n",
    "    \n",
    "\n",
    "# partition the action space of a given environment \n",
    "def partition_action_space(env_name:'string'\n",
    "                           , n_actions:'int'):\n",
    "    \"\"\"function to partitions the action space of an environment into a given number of actions`\"\"\"\n",
    "    \n",
    "    # initialize environment\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # partition the action space to a given number of actions\n",
    "    part_act_space = np.linspace(env.action_space.low[0,0]\n",
    "                                 ,env.action_space.high[0,0],n_actions)\n",
    "    \n",
    "    return part_act_space  \n",
    "\n",
    "\n",
    "########################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
