{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preference-based Policy Iteration (PBPI) Algorithm \n",
    "### Application on the Inverted pendulum problem\n",
    "\n",
    "\n",
    "- This work attempts to reproduce the PBPI algorithm proposed by FÃ¼rnkranz et al., (2012) in their paper \"Preference-based reinforcement learning: a formal framework and a policy iteration algorithm\"\n",
    "- This algorithm is applied on the [inverted pendulum](https://en.wikipedia.org/wiki/Inverted_pendulum) problem. The source code of the environment can be found [here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py)\n",
    "\n",
    "---\n",
    "##### Information about the task/environment.\n",
    "\n",
    "**Description:**\n",
    ">A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "a frictionless track. The pendulum starts upright, and the goal is to\n",
    "prevent it from falling over by increasing and reducing the cart's\n",
    "velocity.\n",
    "\n",
    "**Observation:**\n",
    ">Type: Box(4)\n",
    "\n",
    "|Num|Observation|Min|Max|\n",
    "|--|--|--|--|\n",
    "|0|Cart Position|-4.8|4.8|\n",
    "|1|Cart Velocity|-Inf|Inf|\n",
    "|2|Pole Angle|-0.418 rad (-24 deg)|0.418 rad (24 deg)|\n",
    "|3|Pole Angular Velocity|-Inf|Inf|\n",
    "\n",
    "**Actions:**\n",
    ">Type: Discrete(2)\n",
    "\n",
    "|Num|Action|\n",
    "|--|--|\n",
    "|0|Push cart to the left|\n",
    "|1|Push cart to the right|\n",
    "\n",
    "**Reward:**\n",
    "- Reward is 1 for every step taken, including the termination step.\n",
    "\n",
    "\n",
    "**Starting State:**\n",
    "- All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "\n",
    "**Episode Termination:**\n",
    "- Pole Angle is more than 12 degrees.\n",
    "- Cart Position is more than 2.4 (center of the cart reaches the edge of the display).\n",
    "- Episode length is greater than 200.\n",
    "\n",
    "**Solved Requirements:**\n",
    "- Considered solved when the average return is greater than or equal to 195.0 over 100 consecutive trials.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Questions/Concerns**:\n",
    "- this might only work because the starting states are close to each other\n",
    "- there is no clear way how the learned policy will know which action to take at states which are not in the initial set of sampled states\n",
    "- the computed reward/success graphs only are for the episodes starting from the initial set of starting states? so this doesn't learn a generalized policy?\n",
    "\n",
    "\n",
    "- Performance of the policy alternates (good->bad->good): hypothesis: training dataset shrinks after following a good policy -> results with (new) bad policy -> bad policy collects more data -> results with good policy -> again less training samples..\n",
    "- `LearnLabelRanker` means iteratively train the model using every new batch of training data?\n",
    "\n",
    "\n",
    "**Observations**\n",
    "- There is no clear mention of the exact policy derived, if we constantly pick the ranked #1 - that always gives the same result on the next roll-out. How has the authors balanced exploration vs. exploitation?\n",
    "- More dense the NN becomese, more generalization happens -> high variability in the validation phrase\n",
    "- when the number of training samples are low - the learned policy seems to work better (why? - should the NN overfit the training data to perform better?)\n",
    "\n",
    "\n",
    "\n",
    "- https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T20:39:40.193588Z",
     "start_time": "2020-12-14T20:39:40.189884Z"
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "### importing the necessary packages ###\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import rankdata as rd\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "\n",
    "import io\n",
    "import base64\n",
    "import itertools\n",
    "import tqdm\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T20:39:41.056041Z",
     "start_time": "2020-12-14T20:39:41.050236Z"
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "########## custom environment ##########\n",
    "\n",
    "# maximum length of a trajectory set to 1500 steps\n",
    "gym.envs.register(id='CartPole_PbPI_Version-v0'\n",
    "                  , entry_point='gym.envs.classic_control:CartPoleEnv'\n",
    "                  , max_episode_steps=1500)\n",
    "\n",
    "\n",
    "########## helper functions ##########\n",
    "\n",
    "# generate a random action from a given environment\n",
    "def random_action(environment):\n",
    "    \"\"\" return a random action from the given environment. \"\"\"\n",
    "    \n",
    "    action = environment.action_space.sample()  \n",
    "    return action\n",
    "\n",
    "\n",
    "# generate a list of initial states from a given environment\n",
    "def generate_init_states_S(seed\n",
    "                           , env = 'CartPole_PbPI_Version-v0'):\n",
    "    \"\"\" this function returns a list of randomly generated initial states from a given environment. \"\"\"\n",
    "    \n",
    "    # set the random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # define how many states to generate\n",
    "    n_states = np.random.randint(low=1, high=101) \n",
    "\n",
    "    # define a list to store the generated initial states\n",
    "    init_states_S = []\n",
    "\n",
    "    # create a given environment object\n",
    "    env = gym.make(env)\n",
    "    env.reset()\n",
    "\n",
    "    # create initial states\n",
    "    for _ in range(n_states):\n",
    "\n",
    "        # step through the environment\n",
    "        state, reward, done, info = env.step(env.action_space.sample())  \n",
    "        \n",
    "        # append the geneareted state to list\n",
    "        init_states_S.append(state)\n",
    "            \n",
    "        # if terminates, reset the environment\n",
    "        if done: \n",
    "            env.reset()    \n",
    "      \n",
    "    env.close()\n",
    "            \n",
    "    return init_states_S\n",
    "\n",
    "\n",
    "# partition the action space of a given environment \n",
    "def partition_action_space(env_name:'string'\n",
    "                           , n_actions:'int'):\n",
    "    \"\"\"function to partitions the action space of an environment into a given number of actions`\"\"\"\n",
    "    \n",
    "    # initialize environment\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    # get the action space of the environment\n",
    "    actions = np.arange(env.action_space.n)\n",
    "\n",
    "    # partition the action space to a given number of actions\n",
    "    # - a uniform noise term is added to action signals to make all state transitions non-deterministic\n",
    "    part_act_space = np.linspace(actions[0],actions[-1],n_actions) + np.random.uniform(low = -.2,high=.2) \n",
    "    \n",
    "    return part_act_space  \n",
    "\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T20:39:42.775426Z",
     "start_time": "2020-12-14T20:39:42.751381Z"
    }
   },
   "outputs": [
    {
     "ename": "UnregisteredEnv",
     "evalue": "No registered env with id: MountainCar_PbPI_Version-v0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MountainCar_PbPI_Version-v0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnregisteredEnv\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5ab724f49b61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minit_states_S\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_init_states_S\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MountainCar_PbPI_Version-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m \u001b[0;31m# unwrap the environment to set custom initial states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeprecatedEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Env {} not found (valid versions include {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnregisteredEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No registered env with id: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnregisteredEnv\u001b[0m: No registered env with id: MountainCar_PbPI_Version-v0"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "########### EXTRA : EXAMPLES ###########\n",
    "\n",
    "# generate a list of sample states and plot 20 states\n",
    "init_states_S = generate_init_states_S(16)\n",
    "\n",
    "env = gym.make('CartPole_PbPI_Version-v0')\n",
    "env = env.unwrapped # unwrap the environment to set custom initial states\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "for i in range(20):\n",
    "    env.state = init_states_S[i] # set state\n",
    "    fig.add_subplot(4,5,i+1)\n",
    "    plt.imshow(env.render(mode=\"rgb_array\"))\n",
    "    env.close()\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# example of partitioning the action space\n",
    "print(\"Original action space: \" + str(env.action_space.n))\n",
    "print(\"Partitioned into 3 actions: \" + str(partition_action_space(env_name,3)))\n",
    "print(\"Partitioned action-space mapped to orignal action-space: \" + str([int(round(action,0)) for action in partition_action_space(env_name,3)]))\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Preference-based (approximate) policy iteration algorithm\n",
    "\n",
    "This algorithm includes two main functions and a class:\n",
    " - `evaluate_preference` (func) : this function generates roll-outs to create preferences between pairs of actions at each initial states.\n",
    " - `train_model` (func) : this function uses the generated preference data to estimate the `LabelRanker` model.\n",
    " - `Policy` (class) : this class takes the trained `LabelRanker` model and generate a policy for the agent to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T23:24:05.783137Z",
     "start_time": "2020-12-14T23:24:05.773967Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "### preference generation process ###\n",
    "\n",
    "def evaluate_preference(starting_state # starting state of roll-outs\n",
    "                        , action_1     # first action to execute at the starting-state\n",
    "                        , action_2     # second action to execute at the starting state\n",
    "                        , policy_in    # policy to folow\n",
    "                        , environment_name = 'CartPole_PbPI_Version-v0'   # name of the environment\n",
    "                        , discount_fac = 1        # discounting factor\n",
    "                        , n_rollouts = 20         # number of roll-outs to generate per action\n",
    "                        , max_rollout_len = 1500  # maximum length of a roll-out\n",
    "                        , label_ranker = False    # whether to use the label-ranking model or not\n",
    "                        , p_sig = 0.05            # p-value to use for t-test (to compare returns of roll-outs)\n",
    "                       ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "    \n",
    "        - Roll-outs are generated at each state in the initial state set by starting from the given input action \n",
    "          and following the given policy afterwards. \n",
    "        - Returns of the roll-outs are used to generate preferences for the input action pair.\n",
    "        - Generated preferences are returned to be create a training dataset to learn the LabelRanker model.    \n",
    "    \"\"\"\n",
    "    \n",
    "    # initializing variables\n",
    "    policy = policy_in          \n",
    "    n_rollouts = n_rollouts     \n",
    "    gamma = discount_fac    \n",
    "    s_init = starting_state\n",
    "    max_traj_len = max_rollout_len \n",
    "        \n",
    "    # we store the num. actions executed within the evaluation process (to measure complexity)\n",
    "    action_count = 0 \n",
    "        \n",
    "    # dictionary to store input action values\n",
    "    actions = { 'one' : action_1    \n",
    "              , 'two' : action_2}    \n",
    "\n",
    "    # dictionary to store rewards of roll-outs\n",
    "    r = { 'one' : [None]*n_rollouts \n",
    "        , 'two' : [None]*n_rollouts}  \n",
    "\n",
    "    # dictionary to store average discounted return for each action\n",
    "    avg_r = {}  \n",
    "    \n",
    "    # select each action of the input actions to generate roll-outs:\n",
    "    for action_key, action_value in actions.items():\n",
    "\n",
    "        # generate the defined number of roll-outs for selected action\n",
    "        for rollout in range(n_rollouts):\n",
    "\n",
    "            # create an environment object and set the starting state to the input (initial) state\n",
    "            env = gym.make(environment_name)\n",
    "            env = env.unwrapped\n",
    "            env.state = s_init \n",
    "\n",
    "            # pre-process the action value in case if the action-space is partitioned\n",
    "            # (OpenAI gym environments only accepts discrete action values)\n",
    "            action_val_processed = int(abs(round(action_value)))\n",
    "            observation, reward, done, info = env.step(action_val_processed)\n",
    "            \n",
    "            # define the history variable to store the last observed state\n",
    "            hist = observation \n",
    "            \n",
    "            # add the immediate reward received after executing the action\n",
    "            r[action_key][rollout] = reward  \n",
    "\n",
    "            # follow the given policy to generate a roll-out trajectory \n",
    "            traj_len = 1\n",
    "            while traj_len < max_traj_len and not done: \n",
    "                \n",
    "                # sample next state using the label-ranking model (if TRUE)\n",
    "                if label_ranker: \n",
    "                    observation, reward, done, info = env.step(policy.label_ranking_policy(hist))\n",
    "                    \n",
    "                    # replace current history with the observed state\n",
    "                    hist = observation\n",
    "                    action_count+=1\n",
    "                \n",
    "                # sample next state using a random policy\n",
    "                else: \n",
    "                    observation, reward, done, info = env.step(policy(env))\n",
    "                    action_count+=1\n",
    "\n",
    "                # compute discounted-reward at each step of the roll-out and store the roll-out return\n",
    "                r[action_key][rollout] += (gamma**traj_len) * reward\n",
    "\n",
    "                traj_len += 1\n",
    "\n",
    "            # close the environment after creating roll-outs\n",
    "            env.close()\n",
    "            del env\n",
    "        \n",
    "        # calculate the average discounted returns of the two actions\n",
    "        avg_r[action_key]  = sum(r[action_key]) / len(r[action_key])\n",
    "\n",
    "\n",
    "    # run a t-test to check whether the observed difference between average returns is significant\n",
    "    # (unpaird t-tests: equal variance)\n",
    "    t_val, p_val = stats.ttest_ind(r['one'],r['two']) \n",
    "    \n",
    "    \n",
    "    # return preference information\n",
    "    if (avg_r['one'] > avg_r['two']) and (p_val <= p_sig):\n",
    "        return {'state': s_init\n",
    "               , 'a_j' : actions['one']\n",
    "               , 'a_k' : actions['two']\n",
    "               , 'preference_label' : 1}, action_count\n",
    "    \n",
    "    elif(avg_r['one'] < avg_r['two']) and (p_val <= p_sig):\n",
    "        return {'state': s_init\n",
    "               , 'a_j' : actions['one']\n",
    "               , 'a_k' : actions['two']\n",
    "               , 'preference_label' : 0}, action_count\n",
    "    \n",
    "    # return NaN if avg. returns are not significantly different from each other OR are equal\n",
    "    else: \n",
    "        return {'state': np.nan\n",
    "               , 'a_j' : np.nan\n",
    "               , 'a_k' : np.nan\n",
    "               , 'preference_label' : np.nan}, action_count\n",
    "    \n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T23:24:06.443437Z",
     "start_time": "2020-12-14T23:24:06.426494Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "### LabelRanker Model training process ###\n",
    "\n",
    "def train_model(train_data                  # collection of all preference data\n",
    "                , action_space              # action space of the task\n",
    "                , model_name:str            # name for the model (to store)\n",
    "                , batch_s = 4               # batch size to train the NN model\n",
    "                , mod_layers = [15,7]       # model configuration\n",
    "                , n_epochs = 300            # num. of epochs to train the model\n",
    "                , l_rate = 0.01             # learning rate for the optimization process  \n",
    "                , show_train_plot = False): # flag to display the 'training-loss vs. epoch' plot    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "    \n",
    "        - This function process all preference data to construct a training dataset for the LabelRanker model.\n",
    "        - One training sample takes the form:\n",
    "            X: [state-value (2-D)]\n",
    "            Y: [(normalized) ranking of actions (n-D)], where 'n' is the number of actions in the action space.\n",
    "        - For a given (2-D) state input, the (trained) model, i.e., LabelRanker, predicts the rank of \n",
    "           all possible actions at the input state \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    ### creating the training dataset ###\n",
    "        \n",
    "    # convert training data input to a dataframe | \n",
    "    # remove the rows that have NaN, i.e.,preference evaluations without any action preference\n",
    "    train_df = pd.DataFrame(train_data).dropna()\n",
    "\n",
    "    # create a key for each state in the dataset\n",
    "    # (only select the 'pendulum-velocity & pendulum-angle)\n",
    "    train_df.loc[:, 'state_key'] = train_df.state.apply(lambda x: x[2].astype(str)+\"_\"+x[3].astype(str))\n",
    "\n",
    "    # drop all states (rows) that do not include at least a single preferred action\n",
    "    temp_df1 = train_df.groupby('state_key').preference_label.sum().reset_index()\n",
    "    temp_df1 = temp_df1.loc[temp_df1.preference_label>0] # pick the states that have at least one prefered action\n",
    "    train_df = train_df.merge(right = temp_df1.loc[:,'state_key']\n",
    "                              , right_on = 'state_key'\n",
    "                              , left_on = 'state_key'\n",
    "                              , how = 'right')\n",
    "\n",
    "    \n",
    "    # check if the training dataset is empty \n",
    "    # (if empty, subsequent steps have to be skipped)\n",
    "    if not(train_df.shape[0]>0):\n",
    "        \n",
    "        # if training dataset is emtpy - return None (break the training loop)\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        ### computing action-preference counts for every action (for every states) ###\n",
    "        \n",
    "        # identify the 'prefered-action' at each 'state, action-pair' preference evaluation\n",
    "        train_df.loc[:,'prefered_action'] = train_df.apply(lambda row: row['a_j'] if row['preference_label'] == 1 else row['a_k']  ,axis=1)\n",
    "\n",
    "        # compute the number of times each action is prefered at each state\n",
    "        action_preference_counts = train_df.groupby('state_key').prefered_action.value_counts().unstack()\n",
    "        action_preference_counts.replace(np.nan,0,inplace=True) # if an action is not preferred at a state, set pref. count to '0'\n",
    "\n",
    "        # remove the column index names of the `action_preference_counts' summary table\n",
    "        action_preference_counts.columns.name = None\n",
    "\n",
    "        # find any action(s) that was not preferred at all sampled states \n",
    "        # - this is important because a ranking for every possible action\n",
    "        #   at each state needs to be included in the training (label) data\n",
    "        missed_actions = [action for action in action_space if action not in action_preference_counts.columns.tolist()]\n",
    "        missed_actions = np.array(missed_actions).astype(action_preference_counts.columns.dtype) # convert to the same data-type of remaining columns\n",
    "\n",
    "        # add any missing actions to the `action_preference_counts' table\n",
    "        if len(missed_actions)>0:\n",
    "\n",
    "            # add the missing action (with a preference count of zero)\n",
    "            for action in missed_actions:\n",
    "                action_preference_counts.loc[:,action] = 0\n",
    "\n",
    "            # sort the actions in the summary according to arrangement in action space (ascending order)\n",
    "            action_preference_counts = action_preference_counts.reindex(sorted(action_preference_counts.columns), axis=1)    \n",
    "\n",
    "        \n",
    "        # convert the action-preference-counts (of actions at each state) to a vector and add it as a new column\n",
    "        #  - data in this column is used to create training labels\n",
    "        action_preference_counts.loc[:, 'preference_label_vector'] = pd.DataFrame({'label_data': action_preference_counts.iloc[:,0:].values.tolist()}).values\n",
    "\n",
    "        # append the column having action-preference-counts vectors to the training dataset\n",
    "        train_df = train_df.merge(right = action_preference_counts.loc[:,['preference_label_vector']]\n",
    "                                  , right_index= True\n",
    "                                  , left_on = 'state_key'\n",
    "                                  , how = 'left')\n",
    "        \n",
    "\n",
    "        # create the reduced training dataset \n",
    "        # - drop unnecessary columns & duplicate rows (which have duplicate data for same states)\n",
    "        train_df_reduced = train_df.loc[:,['state', 'state_key', 'preference_label_vector']]\n",
    "        train_df_reduced.drop_duplicates(subset=['state_key'],inplace=True)\n",
    "\n",
    "        print(f'Training data samples: {train_df_reduced.shape[0]}')\n",
    "\n",
    "        \n",
    "        ### preparing the training dataset for the neural network (LabelRanker) model ###\n",
    "\n",
    "        # normalize the action-preference-counts vectors (label data for the model)\n",
    "        # - this step produces the rankings:\n",
    "        # - i.e., the action(s) with the highest preference count(s) will have the highest value(s)\n",
    "        # - after normalization\n",
    "        output_labels_temp = np.array(train_df_reduced.preference_label_vector.tolist())\n",
    "        row_sums = output_labels_temp.sum(axis=1)\n",
    "        output_labels_normalized = output_labels_temp / row_sums[:, np.newaxis]\n",
    "        output_labels = torch.from_numpy(output_labels_normalized) # convert to tensor\n",
    "\n",
    "        # generate the input state data tensors (feature data for the model)\n",
    "        # - this should only include pendulum-angle and pendulum-velocity\n",
    "        input_states  = torch.from_numpy(np.array(train_df_reduced.state.apply(lambda x: [x[2].astype(float),x[3].astype(float)]).tolist())) # only select pole-position and pole-velocity\n",
    "\n",
    "        # create TensorDataset\n",
    "        train_ds = TensorDataset(input_states , output_labels)\n",
    "\n",
    "        # define the batch size\n",
    "        batch_size = batch_s\n",
    "        \n",
    "        # define the data loader\n",
    "        train_dl = DataLoader(train_ds\n",
    "                              , batch_size\n",
    "                              , shuffle=True\n",
    "                              , drop_last=True)\n",
    "\n",
    "            \n",
    "    ### defining and training the neural network (LabelRanker) model ###        \n",
    "    \n",
    "    class Model(nn.Module):\n",
    "\n",
    "        def __init__(self, input_state_len, output_label_len, layers, p=0.3):\n",
    "\n",
    "            super(Model,self).__init__()\n",
    "\n",
    "            all_layers = []\n",
    "            input_size = input_state_len\n",
    "\n",
    "            # create layers\n",
    "            for layer_dim in layers:\n",
    "                all_layers.append(nn.Linear(input_size, layer_dim))\n",
    "                all_layers.append(nn.LeakyReLU(inplace=True))\n",
    "                #all_layers.append(nn.BatchNorm1d(layer_dim))\n",
    "                all_layers.append(nn.Dropout(p))\n",
    "                input_size = layer_dim\n",
    "\n",
    "            all_layers.append(nn.Linear(layers[-1], output_label_len))\n",
    "\n",
    "            self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "        def forward(self, state_vec):\n",
    "            x = self.layers(state_vec)\n",
    "            return x\n",
    "\n",
    "        \n",
    "    # create a NN model instance\n",
    "    model = Model(input_states.shape[1], output_labels.shape[1], mod_layers)\n",
    "\n",
    "    # define optimizer and loss\n",
    "    opt = torch.optim.SGD(model.parameters(), lr = l_rate)\n",
    "    loss_fn = F.mse_loss\n",
    "\n",
    "    # list to store losses\n",
    "    aggregated_losses = []\n",
    "\n",
    "    # defining a function to train the model\n",
    "    def fit(num_epochs, model, loss_fn, opt):\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            for xb,yb in train_dl:\n",
    "\n",
    "                # Generate predictions\n",
    "                pred = model(xb.float())\n",
    "                loss = loss_fn(pred, yb.float())            \n",
    "\n",
    "                # Perform gradient descent\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "\n",
    "            aggregated_losses.append(loss_fn(model(input_states.float()), output_labels.float()).detach().numpy())\n",
    "\n",
    "        print('\\nTraining loss: ', loss_fn(model(input_states.float()), output_labels.float()).detach().numpy(),'\\n')\n",
    "\n",
    "        \n",
    "#     print('Training model...\\n',end='\\r')\n",
    "\n",
    "    # train the model\n",
    "    epochs = n_epochs\n",
    "    fit(epochs, model, loss_fn, opt)\n",
    "\n",
    "    # save the trained model\n",
    "    PATH = f\"./models/{model_name}_pbpi_model.pt\" # path\n",
    "    torch.save(model.state_dict(), PATH) # save\n",
    "    \n",
    "    # plot the model loss\n",
    "    if show_train_plot:\n",
    "        plt.plot(range(epochs), aggregated_losses)\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.show()\n",
    "\n",
    "    # set the model to evaluation mode and return it\n",
    "    return model.eval()\n",
    "\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T23:24:06.969280Z",
     "start_time": "2020-12-14T23:24:06.963505Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "### Derived policy using LabelRanker ###\n",
    "\n",
    "class Policy():\n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "    \n",
    "        - This Policy object takes a given neural network LabelRanker model and uses it to define a policy for the agent to follow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, action_space, model):\n",
    "        self.action_space = action_space # action space of the current environment\n",
    "        self.model = model               # trained NN (LabelRanker) model\n",
    "        \n",
    "    def label_ranking_policy(self,obs):\n",
    "        \"\"\" Produces an action for a given state based on the LabelRanker model prediction\n",
    "            Note: only the pendulum-angle and pendulum-velocity of the input state are considered when producing an action\n",
    "        \n",
    "            At each input state:\n",
    "                - Highest ranked action is selected with a prob. of 0.95\n",
    "                - Second highest ranked action is selected with a prob. of 0.04\n",
    "                - Any remaining actions are selected with an equal proabability of .01 \"\"\"\n",
    "\n",
    "        # only select the pendulum-velocity and angle from the input state vector\n",
    "        state_obs = obs[[2,3]] \n",
    "        state_obs = state_obs.reshape(-1,state_obs.shape[0]) # reshape to be a 2D array\n",
    "        state_obs = torch.from_numpy(state_obs) # convert to a tensor\n",
    "\n",
    "        # make ranking predictions for all actions\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(state_obs.float()) \n",
    "\n",
    "        # rank the indexes of actions (from highest ranked/preferred action to lowest)\n",
    "        ranked_action_idx = (-rd(preds.detach().numpy())).argsort()[:preds.shape[1]]\n",
    "\n",
    "        \n",
    "        ### return the selected action ###\n",
    "        \n",
    "        # if there are more than 2 actions\n",
    "        if len(self.action_space)>2:\n",
    "            \n",
    "            # compute the probabilities for the 3rd action onward\n",
    "            remain_probs = .01/len(ranked_action_idx[2:])\n",
    "            n_remain_actions = ranked_action_idx.shape[0]-2\n",
    "\n",
    "            # select first two (highest preferred actions) 95% and 4% of the time\n",
    "            # select one of the remaining actions 1% time\n",
    "            action = np.random.choice(ranked_action_idx,1 , p=[0.95, 0.04] + list(np.repeat(remain_probs,n_remain_actions)))[0]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # if there are only 2 actions: select highest preferred actions 95% and 5% of the time\n",
    "            action = np.random.choice(ranked_action_idx,1 , p=[.95, 0.05])[0]\n",
    "        \n",
    "        # when action space is partitioned, return the corresponding discretized action\n",
    "        return int(abs(round(self.action_space[int(action)])))\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T23:24:07.534236Z",
     "start_time": "2020-12-14T23:24:07.529162Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "### Evaluating the learned policy ####\n",
    "\n",
    "\n",
    "def run_evaluations(policy               # input policy\n",
    "                    , state_list         # list of initial states\n",
    "                    , step_thresh=100    # step-count (threshold)\n",
    "                    , env_name ='CartPole_PbPI_Version-v0' # name of the environment\n",
    "                    , simulations_per_state = 100 # number of simulations to generate per state\n",
    "                   ):  \n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "    \n",
    "        - For every state in a given list of initial states, 100 simulations are generate and the percentage of\n",
    "           these simulations that exceeds a predefined step-count threadhold (trajectory length) is computed to measure \n",
    "           the performance of the given input policy.\"\"\"\n",
    "    \n",
    "\n",
    "    simu_per_state = simulations_per_state\n",
    "        \n",
    "    # create an environment instance\n",
    "    env_test = gym.make(env_name)\n",
    "    env_test = env_test.unwrapped\n",
    "    \n",
    "    # variable to record the sufficient policy count (across all simulations)\n",
    "    suf_policy_count = 0\n",
    "    \n",
    "    # iterate over all states in the state list\n",
    "    for state in state_list:        \n",
    "        \n",
    "        # generate 100 simulations from each state\n",
    "        for _ in range(simu_per_state):\n",
    "            \n",
    "            # set the starting state and the current observation to the given state \n",
    "            env_test.state, obs = state, state\n",
    "        \n",
    "            # variable to store the return of an episode\n",
    "            return_ep = 0 \n",
    "\n",
    "            # execute 1001 steps in the environment\n",
    "            for _ in range(1001):\n",
    "                action = policy.label_ranking_policy(obs) # generate action from the policy\n",
    "                observation, reward, done, info = env_test.step(action) # execute action\n",
    "                obs = observation     # set history\n",
    "                return_ep += reward   # compute return\n",
    "                if done: break\n",
    "\n",
    "            env_test.reset()\n",
    "            env_test.close()\n",
    "\n",
    "            # increment the sufficient policy count if return exceeds given threshold\n",
    "            # (note: at every step, 1 reward is produced in the environment)\n",
    "            if return_ep>=step_thresh:\n",
    "                suf_policy_count += 1\n",
    "                \n",
    "    # return the aggregated sufficient policy count from the policy evaluation\n",
    "    return (suf_policy_count/(len(state_list)*simu_per_state))\n",
    "\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-14T23:24:07.926Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of initial states: 47\n",
      "\n",
      "\n",
      "Number of actions (per states): 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e8f591fd3740eaab2a06dc8faf4551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=21.0, style=ProgressStyle(description_widâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d5e329277e4d469f3b395ddd34fbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='States', max=47.0, style=ProgressStyle(description_width=â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data samples: 11\n",
      "\n",
      "Training loss:  0.012444023 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454bb1b156404d6b900a931f0dcf3ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='States', max=47.0, style=ProgressStyle(description_width=â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################################################\n",
    "### Preference-Based Policy Iteration Algorithm ###\n",
    "\n",
    "\n",
    "### initializations ###\n",
    "\n",
    "seed = 4 \n",
    "model_name = 'cartPole_original' # name for the LabelRanker model\n",
    "\n",
    "env = gym.make('CartPole_PbPI_Version-v0')   # environment\n",
    "sample_states = generate_init_states_S(seed) # initial state list\n",
    "max_iterr = 21 # max. num. of policy iterations\n",
    "\n",
    "act_space = partition_action_space('CartPole_PbPI_Version-v0',3) # action space\n",
    "\n",
    "act_pairs = list(itertools.combinations(act_space,2)) # generate action-pairs\n",
    "\n",
    "policy = random_action # initial (random) policy\n",
    "\n",
    "agg_pct_l_policies = [] # list to store the evaluation results, i.e., % of learned sufficient policies\n",
    "action_count_li = []    # list to store the action counts in each training iteration\n",
    "\n",
    "label_r_flag = False  # training loop configuration ('use-label-ranker' flag) \n",
    "\n",
    "\n",
    "print(f'\\nNumber of initial states: {len(sample_states)}\\n')\n",
    "print(f'\\nNumber of actions (per states): {len(act_space)}\\n')\n",
    "\n",
    "\n",
    "# track iteration progress\n",
    "pbar = tqdm.notebook.tqdm(total=max_iterr, desc=\"Iteration\", position = 0)\n",
    "\n",
    "iterr = 1\n",
    "while iterr < max_iterr:\n",
    "    \n",
    "    print(f'Iteration {iterr}:')\n",
    "    \n",
    "    train_data = []      # place-holder to store training data\n",
    "    actions_in_iterr = 0 # variable to store the num. actions excuted in each iteration\n",
    "\n",
    "    # track state loop progress\n",
    "    pbar_states = tqdm.notebook.tqdm(total=len(sample_states), desc=\"States\", position = 1, leave=False)\n",
    "    \n",
    "    for state in sample_states:\n",
    "        \n",
    "        for action_pair in act_pairs:\n",
    "            \n",
    "            preference_out, actions_per_pair = evaluate_preference(starting_state = state\n",
    "                                                                 , action_1  = action_pair[0]\n",
    "                                                                 , action_2  = action_pair[1]\n",
    "                                                                 , policy_in = policy\n",
    "                                                                 , label_ranker = label_r_flag\n",
    "                                                                 , n_rollouts = 50\n",
    "                                                                 , p_sig = 0.1)   \n",
    "            \n",
    "            if preference_out is not None:\n",
    "                train_data.append(preference_out)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            # compute the number of actions executed in the iteration loop\n",
    "            actions_in_iterr += actions_per_pair  \n",
    "                    \n",
    "        # update the state loop status\n",
    "        pbar_states.update(1)\n",
    "    \n",
    "    # process training data and learn the LabelRanker model\n",
    "    model = train_model(train_data\n",
    "                        , act_space\n",
    "                        , model_name = model_name\n",
    "                        , mod_layers = [50]\n",
    "                        , batch_s = 4\n",
    "                        , n_epochs = 200\n",
    "                        , l_rate = .5\n",
    "                        , show_train_plot = True\n",
    "                        )\n",
    "    \n",
    "    # when no traiing data is found -> no model returned \n",
    "    # - therefore, break the current iteration loop and continue to the next (after upadting iterr num. and action counts)\n",
    "    if model is None:\n",
    "        \n",
    "        print(f'No training data collected!')\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # compute the cumulative number of actions across iterations\n",
    "        if iterr>1:\n",
    "            action_count_li.append(actions_in_iterr+action_count_li[iterr-2])\n",
    "        else:\n",
    "            action_count_li.append(actions_in_iterr)\n",
    "            \n",
    "        iterr += 1\n",
    "        continue\n",
    "\n",
    "        \n",
    "    # derive a new policy using the trained model\n",
    "    policy = Policy(act_space,model)\n",
    "    \n",
    "    # turn on the 'use-label-ranker' flag\n",
    "    label_r_flag = True\n",
    "    \n",
    "    # compute the cumulative number of actions across iterations\n",
    "    if iterr>1:\n",
    "        action_count_li.append(actions_in_iterr+action_count_li[iterr-2])\n",
    "    else:\n",
    "        action_count_li.append(actions_in_iterr)\n",
    "        \n",
    "    # compute percent. of sufficient policies attained by the latest policy\n",
    "    pct_succ_policies = run_evaluations(policy, sample_states,step_thresh=150)\n",
    "    agg_pct_l_policies.append(pct_succ_policies)\n",
    "    \n",
    "    # plot the 'num. actions' vs. 'pct. of sufficient policies' at every 5 iterations\n",
    "    if iterr%5 == 0:\n",
    "        print(f'Evaluations...')\n",
    "        # clear the last plot data\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "\n",
    "        # plotting rewards over iterations\n",
    "        plt.plot(action_count_li, agg_pct_l_policies )\n",
    "        plt.ylabel('% learned sufficient policies')\n",
    "        plt.xlabel('# actions')\n",
    "        plt.savefig(f'./train_imgs/{model_name}.png') # save the evaluation image\n",
    "        plt.show()  \n",
    "    \n",
    "    # update the algorithm iteration status\n",
    "    pbar.update(1)\n",
    "    iterr += 1\n",
    "        \n",
    "pbar.close()\n",
    "\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "#### Test the learned policy on random starting states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T23:23:21.657757Z",
     "start_time": "2020-12-12T23:23:06.779197Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Test-run for learned policy ###\n",
    "\n",
    "# model path\n",
    "PATH = \"./models/cart_pole_2_states_pbpi_model.pt\"\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "        def __init__(self, input_state_len, output_label_len, layers, p=0.3):\n",
    "\n",
    "            super(Model,self).__init__()\n",
    "\n",
    "            all_layers = []\n",
    "            input_size = input_state_len\n",
    "\n",
    "            # create layers\n",
    "            for layer_dim in layers:\n",
    "                all_layers.append(nn.Linear(input_size, layer_dim))\n",
    "                all_layers.append(nn.LeakyReLU(inplace=True))\n",
    "                #all_layers.append(nn.BatchNorm1d(layer_dim))\n",
    "                all_layers.append(nn.Dropout(p))\n",
    "                input_size = layer_dim\n",
    "\n",
    "            all_layers.append(nn.Linear(layers[-1], output_label_len))\n",
    "\n",
    "            self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "        def forward(self, state_vec):\n",
    "            x = self.layers(state_vec)\n",
    "            return x\n",
    "        \n",
    "# Load\n",
    "model = Model(2, 3, [10])\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "\n",
    "# create policy\n",
    "policy = Policy(act_space,model)\n",
    "\n",
    "# create environment and apply policy\n",
    "env = gym.make('CartPole_PbPI_Version-v0')\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    action = policy.label_ranking_policy(obs)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    obs = observation\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "# observe policy performance\n",
    "video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 1. Training data prep + Model + Model fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T11:42:27.342022Z",
     "start_time": "2020-12-11T11:42:27.320511Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a training dataframe\n",
    "train_df = pd.DataFrame(train_data).dropna()\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T08:20:32.681511Z",
     "start_time": "2020-12-11T08:20:32.677137Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a key for state | only select pole-velocity and pole-position\n",
    "train_df.loc[:, 'state_key'] = train_df.state.apply(lambda x: x[2].astype(str)+\"_\"+x[3].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T08:20:33.018398Z",
     "start_time": "2020-12-11T08:20:33.010186Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# drop all states which does not have any preferred action\n",
    "temp_df1 = train_df.groupby('state_key').preference_label.sum().reset_index()\n",
    "temp_df1 = temp_df1.loc[temp_df1.preference_label>0] # pick the states that have at least one prefered action\n",
    "train_df = train_df.merge(right = temp_df1.loc[:,'state_key']\n",
    "                          , right_on = 'state_key'\n",
    "                          , left_on = 'state_key'\n",
    "                          , how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T08:21:15.623556Z",
     "start_time": "2020-12-11T08:21:15.620703Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# confirm if the training dataset is not empty\n",
    "train_df.shape[0]>0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### THIS CODE IS NOT NEEDED TO TRAIN THE MODEL ####\n",
    "\n",
    "### THERE IS AN ISSUE HERE! - IF ANY OF THE ACTIONS ARE NOT PREFERRED - THAT WILL NOT BE INCLUDED IN THE UNIQUE ACTIONS###\n",
    "### - MANUALLY PASS ALL ACTIONS IN THE ACTION SPACE TO GET THE UNIQUE SET OF ACTIONS\n",
    "\n",
    "# create a single column with the set of actions executed per state\n",
    "temp_df1 = train_df.groupby('state_key')['a_j'].unique().reset_index()\n",
    "temp_df2 = train_df.groupby('state_key')['a_k'].unique().reset_index()\n",
    "temp_df3 = temp_df1.merge(right=temp_df2\n",
    "                          , right_on = 'state_key'\n",
    "                          , left_on = 'state_key'\n",
    "                          , how = 'inner')\n",
    "temp_df3.loc[:,'unique_acts'] = temp_df3.apply(lambda row: set(list(row['a_j']) + list(row['a_k'])) ,axis=1)\n",
    "\n",
    "temp_df3.head(2)\n",
    "\n",
    "# add the unique-action column to train dataset\n",
    "train_df = train_df.merge(right = temp_df3.loc[:,['state_key', 'unique_acts']]\n",
    "              , right_on =  'state_key'\n",
    "              , left_on = 'state_key'\n",
    "              , how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T08:20:42.199084Z",
     "start_time": "2020-12-11T08:20:42.161762Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a 'prefered-action' value for each state\n",
    "train_df.loc[:,'prefered_action'] = train_df.apply(lambda row: row['a_j'] if row['preference_label'] == 1 else row['a_k']  ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T08:19:28.046270Z",
     "start_time": "2020-12-11T08:19:28.040727Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T12:31:19.313009Z",
     "start_time": "2020-12-10T12:31:19.303493Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# count the number of times each action (IN THE ACTION SPACE) is prefered at a state\n",
    "action_preference_counts = train_df.groupby('state_key').prefered_action.value_counts().unstack()\n",
    "action_preference_counts.replace(np.nan,0,inplace=True)\n",
    "action_preference_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T12:31:20.369303Z",
     "start_time": "2020-12-10T12:31:20.364081Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove the column index nameS\n",
    "action_preference_counts.columns.name = None\n",
    "\n",
    "# find actions not selected as a preferred action at any state \n",
    "missed_actions = [action for action in act_space if action not in action_preference_counts.columns.tolist()]\n",
    "missed_actions = np.array(missed_actions).astype(action_preference_counts.columns.dtype) # convert to the same data-type of columns\n",
    "\n",
    "# add any missing actions to the summary table\n",
    "if len(missed_actions)>0:\n",
    "    \n",
    "    # add the missing action (with 0 preference value)\n",
    "    for action in missed_actions:\n",
    "        action_preference_counts.loc[:,action] = 0\n",
    "\n",
    "    # sort the actions in the summary according to arrangement in action space (ascending order)\n",
    "    action_preference_counts = action_preference_counts.reindex(sorted(action_preference_counts.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T12:31:21.829397Z",
     "start_time": "2020-12-10T12:31:21.822087Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "action_preference_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T12:33:34.528098Z",
     "start_time": "2020-12-10T12:33:34.523589Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### convert the action-preference-counts to a vector (to be used as training labels)\n",
    "action_preference_counts.loc[:, 'preference_label_vector'] = pd.DataFrame({'label_data': action_preference_counts.iloc[:,0:].values.tolist()}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T12:33:36.640747Z",
     "start_time": "2020-12-10T12:33:36.632105Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "action_preference_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add preference label vector to training dataset\n",
    "train_df = train_df.merge(right = action_preference_counts.loc[:,['preference_label_vector']]\n",
    "                          , right_index= True\n",
    "                          , left_on = 'state_key'\n",
    "                          , how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create the reduced training dataset (drop unnecessary columns + duplicate rows: which have data for same state)\n",
    "train_df_reduced = train_df.loc[:,['state', 'state_key', 'unique_acts', 'preference_label_vector']]\n",
    "\n",
    "train_df_reduced.drop_duplicates(subset=['state_key'],inplace=True)\n",
    "train_df_reduced.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# above dataset has - state + list of actions + # preferences made for each action (at state) --> each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output_labels_temp = np.array(train_df_reduced.preference_label_vector.tolist())\n",
    "\n",
    "# to handle 1-D output label vector\n",
    "if len(output_labels_temp.shape) == 1:\n",
    "    output_labels_normalized = output_labels_temp\n",
    "else:\n",
    "    row_sums = output_labels_temp.sum(axis=1)\n",
    "    output_labels_normalized = output_labels_temp / row_sums[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create training dataset\n",
    "input_states  = torch.from_numpy(np.array(train_df_reduced.state.apply(lambda x: x.astype(float)).tolist()))\n",
    "output_labels = torch.from_numpy(output_labels_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import tensor dataset & data loader\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(input_states , output_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define data loader\n",
    "batch_size = 3\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, drop_last=True)\n",
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, input_state_len, output_label_len, layers, p=0.4):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.batch_norm_num = nn.BatchNorm1d(input_state_len)\n",
    "\n",
    "        all_layers = []\n",
    "        input_size = input_state_len\n",
    "\n",
    "        # create layers\n",
    "        for layer_dim in layers:\n",
    "            all_layers.append(nn.Linear(input_size, layer_dim))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(layer_dim))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = layer_dim\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_label_len))\n",
    "\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, state_vec):\n",
    "        x = self.batch_norm_num(state_vec)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = Model(input_states.shape[1], output_labels.shape[1], [100,500,50], p=0.4)\n",
    "# model = Model(input_states.shape[1], 1 if len(output_labels.shape)== 1 else output_labels.shape[1], [100,500,50], p=0.4) # for 1-D targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr = 1e-1)\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "for xb,yb in train_dl:\n",
    "    print(yb.reshape(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# to store losses\n",
    "aggregated_losses = []\n",
    "\n",
    "# Define a utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # Generate predictions\n",
    "            pred = model(xb.float())\n",
    "            #loss = loss_fn(pred, yb.float())            \n",
    "            loss = loss_fn(pred, yb.reshape(3,1).float()) \n",
    "            \n",
    "            # Perform gradient descent\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        #aggregated_losses.append(loss_fn(model(input_states.float()), output_labels.float()).detach().numpy())\n",
    "        \n",
    "        if epoch%25 == 0:\n",
    "                print(f'epoch: {epoch:3} loss: {loss.item():10.8f}')\n",
    "            \n",
    "    print('Training loss: ', loss_fn(model(input_states.float()), output_labels.float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xb,yb = next(iter(train_dl))\n",
    "pred = model(xb.float())\n",
    "#loss_fn(pred, yb.float())  \n",
    "yb.reshape(3,1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "epochs= 300\n",
    "fit(epochs, model, loss_fn, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), aggregated_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2. Label Ranking Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "\n",
    "state_obs = observation[[2,3]] # only select angel and angular velo. of pendulum from state vector\n",
    "state_obs = state_obs.reshape(-1,state_obs.shape[0]) # reshape to be a 2D array\n",
    "state_obs = torch.from_numpy(state_obs) # convert to a tensor\n",
    "\n",
    "# make the prediction for actions\n",
    "with torch.no_grad():\n",
    "    preds = model(state_obs.float()) \n",
    "    \n",
    "# rank the indexes of actions (from highest ranked/preferred action to lowest)\n",
    "ranked_action_idx = (-rd(preds.detach().numpy())).argsort()[:preds.shape[1]]\n",
    "\n",
    "# return the action value\n",
    "remain_probs = .1/len(ranked_action_idx[2:])\n",
    "n_remain_actions = ranked_action_idx.shape[0]-2\n",
    "\n",
    "# - select first two (highest preferred actions) 80% and 10% of the time\n",
    "# - select one of the remaining actions 10% time\n",
    "action = np.random.choice(ranked_action_idx,1 , p=[0.8, 0.1] + list(np.repeat(remain_probs,n_remain_actions)))[0]\n",
    "\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 3. Visualize episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### METHOD 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar_PbPI_Version-v0')\n",
    "env = wrappers.Monitor(env, \"./gym-results/mount-car\", force=True)\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(random_action(env))\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "video = io.open('./gym-results/mount-car/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### METHOD 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nb_episodes = 20\n",
    "nb_timesteps = 100\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "\n",
    "for episode in range(nb_episodes):  # iterate over the episodes\n",
    "    state = env.reset()             # initialise the environment\n",
    "    rewards = []\n",
    "    \n",
    "    for t in range(nb_timesteps):    # iterate over time steps\n",
    "        #env.render()                 # display the environment\n",
    "        img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, info = env.step(0)  # implement the action chosen by the policy\n",
    "        rewards.append(reward)      # add 1 to the rewards list\n",
    "        \n",
    "        if done: # the episode ends either if the pole is > 15 deg from vertical or the cart move by > 2.4 unit from the centre\n",
    "            cumulative_reward = sum(rewards)\n",
    "            print(\"episode {} finished after {} timesteps. Total reward: {}\".format(episode, t+1, cumulative_reward))  \n",
    "            break\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
