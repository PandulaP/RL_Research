{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preference-based Policy Iteration Algorithm - Application on Inverted Pendulum Problem\n",
    "\n",
    "This is an attempt to replicate the work done by FÃ¼rnkranz et al., (2012) in their paper \"Preference-based reinforcement learning: a formal framework and a policy iteration algorithm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TO-DOs:*\n",
    ">- Run t-tests for pair-wise comparisons (only add preferences for significantly different pairs) -> might help to converge. -->\n",
    "\n",
    "*ISSUES/UNCLEAR POINTS:*\n",
    ">- how to handle the two state values that were not considered? (cart-position and velocity)\n",
    "- how the NN (approximator) actually makes the prediction?\n",
    "- how does the environment execute actions in the extended action space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import io\n",
    "import base64\n",
    "import itertools\n",
    "import tqdm\n",
    "\n",
    "from scipy.stats import rankdata as rd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State observation**:\n",
    "    \n",
    "    Type: Box(4)\n",
    "\n",
    "    Num     Observation               Min                     Max\n",
    "    0       Cart Position             -4.8                    4.8\n",
    "    1       Cart Velocity             -Inf                    Inf\n",
    "    2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "    3       Pole Angular Velocity     -Inf                    Inf\n",
    "\n",
    "- For this project, I will describe the state of the pendulum using only the angle and angular velocity of the pole, ignoring the position and the velocity of cart.\n",
    "\n",
    "**Actions**:\n",
    "\n",
    "    Type: Discrete(2)\n",
    "    \n",
    "    Num   Action\n",
    "    0     Push cart to the left\n",
    "    1     Push cart to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 2\n",
      "Observation space: Box(4,)\n",
      "Max. values of observation space:[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "Min. values of observation space:[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "print(\"Number of actions: \" + str(env.action_space.n))\n",
    "print(\"Observation space: \" + str(env.observation_space))\n",
    "print(\"Max. values of observation space:\" + str(env.observation_space.high))\n",
    "print(\"Min. values of observation space:\" + str(env.observation_space.low))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def state_filter(state):\n",
    "    \"\"\" only use the angle and the angular velocity of the pole to describe the state\"\"\"\n",
    "    \n",
    "    return state[[2,3]]\n",
    "\n",
    "\n",
    "def random_action(observation, model=None):\n",
    "    \"\"\" return a random action: either 0 (left) or 1 (right)\"\"\"\n",
    "    \n",
    "    action = env.action_space.sample()  \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I generate random samples $S$ in this setting by simulating a uniform random number (max 100) of uniform random actions from the initial state. If the pendulum fell within this sequence, the procedure was repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_init_states_S(seed, filter_state=True):\n",
    "    \"\"\"this function returns a list of randomly generated initial states from the CartPole-v0 environment \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n_actions = np.random.randint(low=1, high=101)                # how many actions to generate\n",
    "    seq_actions = np.random.randint(low=0,high=2,size=n_actions)  # random sequence of actions\n",
    "\n",
    "    init_states_S = []   # to store initial states\n",
    "\n",
    "    env = gym.make('CartPole-v0')\n",
    "    env.reset()\n",
    "\n",
    "    for action in seq_actions:\n",
    "\n",
    "        state, reward, done, info = env.step(action)  # implement the actions in the random sequence\n",
    "        \n",
    "        if filter_state:\n",
    "            init_states_S.append(state_filter(state)) # append the environment state to list (only angular velocity and angle)\n",
    "        else:\n",
    "            init_states_S.append(state) # all 4 state observations\n",
    "            \n",
    "        if done: # the episode ends either if the pole is > 15 deg from vertical or the cart move by > 2.4 unit from the centre\n",
    "            env.reset()    \n",
    "            \n",
    "    env.close()\n",
    "            \n",
    "    return init_states_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Generate a sequence of intial states (for $S$) and display first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_states_S = generate_init_states_S(16) # randomly generated states initial\n",
    "\n",
    "env = gym.make('CartPole-v0') # create inv. pend environment\n",
    "env = env.unwrapped           # unwrap the environment to send custom initial states\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "for i in range(10):\n",
    "    env.state = np.concatenate((np.array([0.0,0.0]),init_states_S[i])) # manually adding cart-posi & cart-velocity\n",
    "    fig.add_subplot(2,5,i+1)\n",
    "    plt.imshow(env.render(mode=\"rgb_array\"))\n",
    "    env.close()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I partition the action space to create multiple actions, because in these cases it becomes less and less likely that a unique best actions can be found. The range of the original action set {0, 1} is partitioned equidistantly into the given number of actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_action_space(env,n_actions):\n",
    "    \"\"\"this function partitions the action space of a given environment into a given number of `n_actions`\"\"\"\n",
    "    \n",
    "    actions = np.arange(env.action_space.n)\n",
    "\n",
    "    # a uniform noise term is added to action signals to make all state transitions non-deterministic\n",
    "    part_act_space = np.linspace(actions[0],actions[-1],n_actions) + np.random.uniform(low = -.2,high=.2) \n",
    "    \n",
    "    return part_act_space                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original action space: 2\n",
      "Partitioned into 3 actions: [0.12849278 0.62849278 1.12849278]\n",
      "Partitioned into 9 actions: [-0.1763552 -0.0513552  0.0736448  0.1986448  0.3236448  0.4486448\n",
      "  0.5736448  0.6986448  0.8236448]\n",
      "Partitioned into 12 actions: [-0.1201935  -0.02928441  0.06162468  0.15253378  0.24344287  0.33435196\n",
      "  0.42526105  0.51617014  0.60707923  0.69798832  0.78889741  0.8798065 ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "print(\"Original action space: \" + str(env.action_space.n))\n",
    "print(\"Partitioned into 3 actions: \" + str(partition_action_space(env,3)))\n",
    "print(\"Partitioned into 9 actions: \" + str(partition_action_space(env,9)))\n",
    "print(\"Partitioned into 12 actions: \" + str(partition_action_space(env,12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preference-based Approximate Policy Iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "### Evaluate-Preference procedure ###\n",
    "\n",
    "### Description:\n",
    "###  This is done using roll-outs\n",
    "###  At every state in the initial state set, roll-outs are generated from each action for the same policy\n",
    "###  Accumulated rewards from each roll-out from each action are used to generate preferences for every pair of actions\n",
    "###  Generated preferences are stored in the training set; training data are used to learn the LabelRanker\n",
    "\n",
    "### Functionality:\n",
    "### - INPUTS  : starting state (s), two-actions(a_k, a_j), (current) policy (\\pi), max. length of trajectoris (L)\n",
    "### - PROCESS : generate roll-out (fixed time horizon) and calculate accumulate reward\n",
    "### - OUTPUT  : compare return from each rollout: store preference info in training dataset as (s,a_k > a_j)\n",
    "\n",
    "### - Run this procedure for every action-pair at all initial-states\n",
    "\n",
    "def evaluate_preference(starting_state\n",
    "                        , action_1\n",
    "                        , action_2\n",
    "                        , policy\n",
    "                        , environment_name = 'CartPole-v0'\n",
    "                        , discount_fac = 1\n",
    "                        , n_rollouts = 20\n",
    "                        , max_rollout_len = 1500\n",
    "                        , label_ranker=False\n",
    "                        , p_sig = 0.05\n",
    "                       ):\n",
    "    \n",
    "    \n",
    "    policy = policy              # policy to follow in roll-outs\n",
    "    n_rollouts = n_rollouts      # number of roll-outs to generate\n",
    "    gamma = discount_fac         # discount factor\n",
    "\n",
    "    # state is 4 dimensional by default: if only 2 give, has to expand to 4 dimensions\n",
    "    if len(starting_state) == 2:\n",
    "        # manually define state-values for 'cart-position' & 'cart-velocity'\n",
    "        cart_posi, cart_velo = 0,0   \n",
    "        s_init = np.concatenate((np.array([cart_posi,cart_velo]),starting_state))\n",
    "    else:\n",
    "        s_init = starting_state\n",
    "        \n",
    "    # dict. to store actions\n",
    "    actions = { 'one' : action_1 \n",
    "              , 'two' : action_2}    \n",
    "\n",
    "    # dict to store  rewards of roll-outs starting from each action\n",
    "    r = { 'one' : [None]*n_rollouts \n",
    "        , 'two' : [None]*n_rollouts}  \n",
    "\n",
    "    # dict to store average discounted return for for each action\n",
    "    avg_r = {}  \n",
    "\n",
    "    max_traj_len = max_rollout_len # maximum roll-out trajectory length\n",
    "\n",
    "    for action_key, action_value in actions.items():\n",
    "\n",
    "        # generate roll-outs\n",
    "        for rollout in range(n_rollouts):\n",
    "\n",
    "            env = gym.make(environment_name)\n",
    "            env = env.unwrapped\n",
    "            \n",
    "            rollout_reward = []\n",
    "\n",
    "            env.state = s_init  # set the starting state\n",
    "\n",
    "            # absolute of the rounded action value is applied in case action space is partitioned\n",
    "            observation, reward, done, info = env.step(int(abs(round(action_value))))\n",
    "            hist = observation\n",
    "            \n",
    "            r[action_key][rollout] = reward # add the immediate reward of the action\n",
    "\n",
    "\n",
    "            traj_len = 1\n",
    "            while traj_len < max_traj_len and not done:\n",
    "                \n",
    "                # select policy: random vs label-ranker\n",
    "                if label_ranker:\n",
    "                    observation, reward, done, info = env.step(policy.label_ranking_policy(observation)) # label-ranker\n",
    "                    hist = observation\n",
    "                else:\n",
    "                    observation, reward, done, info = env.step(policy(observation)) #random-policy\n",
    "\n",
    "                r[action_key][rollout] += (gamma**traj_len) * reward\n",
    "\n",
    "                traj_len += 1\n",
    "\n",
    "            #env.reset()\n",
    "            env.close()\n",
    "            del env\n",
    "\n",
    "        # calculate average discounted return \n",
    "        avg_r[action_key]  = sum(r[action_key]) / len(r[action_key])\n",
    "\n",
    "    ### TO-DO ###\n",
    "    # RUN T-TEST AND RETURN ACTION PREFERENCE ONLY IF THERE IS A SIGNIFICANT DIFFERENCE\n",
    "    \n",
    "    t_val, p_val = stats.ttest_rel(r['one'],r['two'])\n",
    "    \n",
    "    # return preference info. to generate training data\n",
    "    if (avg_r['one'] > avg_r['two']) and (p_val < p_sig):\n",
    "        return {'state': s_init[[2,3]] if len(s_init)>2 else s_init # only use the angel and velocity of pendulum for state\n",
    "               , 'a_j' : actions['one']\n",
    "               , 'a_k' : actions['two']\n",
    "               , 'preference_label' : 1}\n",
    "    elif(avg_r['one'] < avg_r['two']) and (p_val < p_sig):\n",
    "        return {'state': s_init[[2,3]] if len(s_init)>2 else s_init # only use the angel and velocity of pendulum for state\n",
    "               , 'a_j' : actions['one']\n",
    "               , 'a_k' : actions['two']\n",
    "               , 'preference_label' : 0}\n",
    "    else:\n",
    "        return {'state': np.nan\n",
    "               , 'a_j' : np.nan\n",
    "               , 'a_k' : np.nan\n",
    "               , 'preference_label' : np.nan}\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "evaluate_preference(env.reset(),0.3,0.6,random_action,label_ranker=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "### LABEL-RANKER MODEL-TRAINING procedure ###\n",
    "\n",
    "### Description:\n",
    "###  XXX..\n",
    "\n",
    "### Functionality:\n",
    "### - INPUTS  : Xxx\n",
    "### - PROCESS : Xxx\n",
    "### - OUTPUT  : Xxx\n",
    "\n",
    "### FUNCTION TO CREATE TRAINING DATASET + MODEL + RETURN THE TRAINED MODEL ###\n",
    "\n",
    "def train_model(train_data\n",
    "                , batch_s = 3\n",
    "                , mod_layers = [100,500,50]\n",
    "                , n_epochs = 300):\n",
    "\n",
    "    ###########################################\n",
    "    ###### CONSTRUCTING TRAINING-DATASET ######\n",
    "        \n",
    "        \n",
    "    # create a training dataframe\n",
    "    train_df = pd.DataFrame(train_data).dropna() # remove np.nan rows while creating dataframe\n",
    "\n",
    "    # create a key for each state\n",
    "    train_df.loc[:, 'state_key'] = train_df.state.apply(lambda x: x[0].astype(str)+\"_\"+x[1].astype(str))\n",
    "\n",
    "    # drop all states (rows) which does not include any preferred action\n",
    "    temp_df1 = train_df.groupby('state_key').preference_label.sum().reset_index()\n",
    "    temp_df1 = temp_df1.loc[temp_df1.preference_label>0] # pick the states that have at least one prefered action\n",
    "    train_df = train_df.merge(right = temp_df1.loc[:,'state_key']\n",
    "                              , right_on = 'state_key'\n",
    "                              , left_on = 'state_key'\n",
    "                              , how = 'right')\n",
    "\n",
    "    ## create a single column to include the set of all actions executed at each state\n",
    "    temp_df1 = train_df.groupby('state_key')['a_j'].unique().reset_index()\n",
    "    temp_df2 = train_df.groupby('state_key')['a_k'].unique().reset_index()\n",
    "    temp_df3 = temp_df1.merge(right=temp_df2\n",
    "                              , right_on = 'state_key'\n",
    "                              , left_on = 'state_key'\n",
    "                              , how = 'inner')\n",
    "    temp_df3.loc[:,'unique_acts'] = temp_df3.apply(lambda row: set(list(row['a_j']) + list(row['a_k'])) ,axis=1)\n",
    "\n",
    "    # add the unique-action column to train dataset\n",
    "    train_df = train_df.merge(right = temp_df3.loc[:,['state_key', 'unique_acts']]\n",
    "                  , right_on =  'state_key'\n",
    "                  , left_on = 'state_key'\n",
    "                  , how = 'left')\n",
    "\n",
    "    # create a 'prefered-action' value for each state, action-preference pair\n",
    "    train_df.loc[:,'prefered_action'] = train_df.apply(lambda row: row['a_j'] if row['preference_label'] == 1 else row['a_k']  ,axis=1)\n",
    "\n",
    "    # count the number of times each action is prefered at a state\n",
    "    action_preference_counts = train_df.groupby('state_key').prefered_action.value_counts().unstack()\n",
    "    action_preference_counts.replace(np.nan,0,inplace=True) # if an action is not preferred every, set it to '0'\n",
    "\n",
    "    # convert the action-preference-counts to a vector and add as a new column (to be used as training labels)\n",
    "    action_preference_counts.loc[:, 'preference_label_vector'] = action_preference_counts.iloc[:,0:].values.tolist()\n",
    "\n",
    "    # add preference label vector to training dataset\n",
    "    train_df = train_df.merge(right = action_preference_counts.loc[:,['preference_label_vector']]\n",
    "                              , right_index= True\n",
    "                              , left_on = 'state_key'\n",
    "                              , how = 'left')\n",
    "\n",
    "    # create the reduced training dataset (drop unnecessary columns + duplicate rows: which have data for same state)\n",
    "    train_df_reduced = train_df.loc[:,['state', 'state_key', 'unique_acts', 'preference_label_vector']]\n",
    "    train_df_reduced.drop_duplicates(subset=['state_key'],inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    ###### PREPARING TRAINING-DATA TENSORS (FOR MODEL) ######\n",
    "\n",
    "    # normalizing the target (preference label) vectors\n",
    "    output_labels_temp = np.array(train_df_reduced.preference_label_vector.tolist())\n",
    "\n",
    "    # to handle 1-D output label vector\n",
    "    if len(output_labels_temp.shape) == 1:\n",
    "        output_labels_normalized = output_labels_temp\n",
    "    else:\n",
    "        row_sums = output_labels_temp.sum(axis=1)\n",
    "        output_labels_normalized = output_labels_temp / row_sums[:, np.newaxis]\n",
    "\n",
    "    # creating training data tensors\n",
    "    input_states  = torch.from_numpy(np.array(train_df_reduced.state.apply(lambda x: x.astype(float)).tolist()))\n",
    "    output_labels = torch.from_numpy(output_labels_normalized)\n",
    "\n",
    "    # create TensorDataset\n",
    "    train_ds = TensorDataset(input_states , output_labels)\n",
    "\n",
    "    # define data loader\n",
    "    batch_size = batch_s\n",
    "    train_dl = DataLoader(train_ds, batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "    ###### CREATING NEURAL NETWORK FUNCTION APPROXIMATOR (MODEL) ######\n",
    "\n",
    "    class Model(nn.Module):\n",
    "\n",
    "        def __init__(self, input_state_len, output_label_len, layers, p=0.4):\n",
    "\n",
    "            super().__init__()\n",
    "            self.batch_norm_num = nn.BatchNorm1d(input_state_len)\n",
    "\n",
    "            all_layers = []\n",
    "            input_size = input_state_len\n",
    "\n",
    "            # create layers\n",
    "            for layer_dim in layers:\n",
    "                all_layers.append(nn.Linear(input_size, layer_dim))\n",
    "                all_layers.append(nn.ReLU(inplace=True))\n",
    "                all_layers.append(nn.BatchNorm1d(layer_dim))\n",
    "                all_layers.append(nn.Dropout(p))\n",
    "                input_size = layer_dim\n",
    "\n",
    "            all_layers.append(nn.Linear(layers[-1], output_label_len))\n",
    "\n",
    "            self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "        def forward(self, state_vec):\n",
    "            x = self.batch_norm_num(state_vec)\n",
    "            x = self.layers(x)\n",
    "            return x\n",
    "\n",
    "    # define model instance\n",
    "    model = Model(input_states.shape[1], output_labels.shape[1], mod_layers, p=0.4)\n",
    "\n",
    "    opt = torch.optim.SGD(model.parameters(), lr = 1e-1)\n",
    "    loss_fn = F.mse_loss\n",
    "\n",
    "\n",
    "    ###### DEFINING FIT-FUNCTION TO TRAIN THE MODEL ######\n",
    "\n",
    "#     print('Training model...\\n',end='\\r')\n",
    "\n",
    "    # to store losses\n",
    "    aggregated_losses = []\n",
    "\n",
    "    # Define a utility function to train the model\n",
    "    def fit(num_epochs, model, loss_fn, opt):\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for xb,yb in train_dl:\n",
    "\n",
    "                # Generate predictions\n",
    "                pred = model(xb.float())\n",
    "                loss = loss_fn(pred, yb.float())            \n",
    "\n",
    "                # Perform gradient descent\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "\n",
    "            aggregated_losses.append(loss_fn(model(input_states.float()), output_labels.float()).detach().numpy())\n",
    "\n",
    "#             if epoch%100 == 0:\n",
    "#                     print(f'epoch: {epoch:3} loss: {loss.item():10.8f}')\n",
    "#       print('\\nTraining loss: ', loss_fn(model(input_states.float()), output_labels.float()).detach().numpy(),'\\n', end = '\\r')\n",
    "\n",
    "\n",
    "    ###### TRAINING-DATA THE MODEL ######\n",
    "\n",
    "    epochs = n_epochs\n",
    "    fit(epochs, model, loss_fn, opt)\n",
    "    \n",
    "    \n",
    "    ###### SAVE MODEL (for inference) ######\n",
    "    \n",
    "    PATH = \"./models/cart_pole_pbpi_model.pt\" # path\n",
    "    torch.save(model.state_dict(), PATH) # save\n",
    "\n",
    "#     # plotting model loss\n",
    "#     plt.plot(range(epochs), aggregated_losses)\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.show()\n",
    "\n",
    "    # set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    #return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = train_model(train_data)\n",
    "\n",
    "# making a sample prediction\n",
    "with torch.no_grad():\n",
    "    preds = model(input_states.float())\n",
    "    \n",
    "a = np.random.randint(10)\n",
    "print(f' - Prediction: {preds[a]}\\n - Target: {output_labels[a]}')\n",
    "# No need to worry about absolute values: only interested in the rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "### LABEL RANKER (POLICY) ###\n",
    "\n",
    "# This takes the trained model as an input\n",
    "# When a new state is given, the model makes a prediction for each action\n",
    "# This function then rank the actions based on the prediction\n",
    "# I select the Highest ranked (most prefered action) 90% time and second most preferred action 10% time\n",
    "\n",
    "class Policy():\n",
    "    \n",
    "    def __init__(self, action_space, model):\n",
    "        self.action_space = action_space\n",
    "        self.model = model\n",
    "        \n",
    "    def label_ranking_policy(self,obs):\n",
    "\n",
    "        state_obs = obs[[2,3]] # only select angel and angular velo. of pendulum from state vector\n",
    "        state_obs = state_obs.reshape(-1,state_obs.shape[0]) # reshape to be a 2D array\n",
    "        state_obs = torch.from_numpy(state_obs) # convert to a tensor\n",
    "\n",
    "        # make the prediction for actions\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(state_obs.float()) \n",
    "\n",
    "        # rank the indexes of actions (from highest ranked/preferred action to lowest)\n",
    "        ranked_action_idx = (-rd(preds.detach().numpy())).argsort()[:preds.shape[1]]\n",
    "\n",
    "        # return the action value\n",
    "        remain_probs = .1/len(ranked_action_idx[2:])\n",
    "        n_remain_actions = ranked_action_idx.shape[0]-2\n",
    "\n",
    "        # - select first two (highest preferred actions) 80% and 10% of the time\n",
    "        # - select one of the remaining actions 10% time\n",
    "        action = np.random.choice(ranked_action_idx,1 , p=[0.8, 0.1] + list(np.repeat(remain_probs,n_remain_actions)))[0]\n",
    "        \n",
    "        # when action space is divided, return the correct action\n",
    "        return int(abs(round(self.action_space[int(action)])))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "observation = env.reset()\n",
    "act_space = partition_action_space(env,4)\n",
    "\n",
    "policy = Policy(act_space,model)\n",
    "\n",
    "policy.label_ranking_policy(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of initial states: 100\n",
      "\n",
      "\n",
      "Number of actions (per states): 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1892: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for 5 episodes: 11.2\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â                                                                                 | 1/100 [00:27<44:50, 27.17s/it]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must have equal len keys and value when setting with an ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-381902161b80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# process training data and learn a model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# derive a new policy using trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-ee58e5f82738>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(train_data, batch_s, mod_layers, n_epochs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# convert the action-preference-counts to a vector and add as a new column (to be used as training labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0maction_preference_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'preference_label_vector'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_preference_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# add preference label vector to training dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    379\u001b[0m                             \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m                         )\n\u001b[1;32m--> 381\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m                         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    525\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                         raise ValueError(\n\u001b[1;32m--> 527\u001b[1;33m                             \u001b[1;34m\"Must have equal len keys and value \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m                             \u001b[1;34m\"when setting with an ndarray\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                         )\n",
      "\u001b[1;31mValueError\u001b[0m: Must have equal len keys and value when setting with an ndarray"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "### Preference-Based Policy Iteration Algorithm ###\n",
    "\n",
    "### Functionality:\n",
    "### - INPUTS  : sample states (s'), initial (random) policy (\\pi0), max. num. of policy iterations (p)\n",
    "### - PROCESS : generate roll-out (fixed time horizon) and calculate accumulate reward\n",
    "### - OUTPUT  : compare return from each rollout: store preference info in training dataset as (s,a_k > a_j)\n",
    "\n",
    "\n",
    "# initialize environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# sample states\n",
    "seed = 20 #np.random.randint(1000)\n",
    "sample_states = generate_init_states_S(seed = seed) # randomly generated states initial\n",
    "\n",
    "print(f'\\nNumber of initial states: {len(sample_states)}\\n')\n",
    "\n",
    "# maximum number of policy iterations\n",
    "max_iterr = 100\n",
    "\n",
    "# action space to consider\n",
    "act_space = partition_action_space(env,3)\n",
    "print(f'\\nNumber of actions (per states): {len(act_space)}\\n')\n",
    "\n",
    "# generate action-pairs (per each)\n",
    "act_pairs = list(itertools.combinations(act_space,2))\n",
    "\n",
    "\n",
    "iterr = 1\n",
    "\n",
    "pbar = tqdm.tqdm(total=max_iterr)\n",
    "\n",
    "# initial random policy\n",
    "policy_init = random_action\n",
    "policy = policy_init\n",
    "\n",
    "lr_flag = False # pass empty value for Model \n",
    "while iterr < max_iterr:\n",
    "    \n",
    "    # place-holder for training data\n",
    "    train_data = [] # uncomment this after adding LabelRanker\n",
    "\n",
    "    for state in sample_states:\n",
    "        \n",
    "        for action_pair in act_pairs:\n",
    "            preference_out = evaluate_preference(state\n",
    "                                                 , action_pair[0]\n",
    "                                                 , action_pair[1]\n",
    "                                                 , policy\n",
    "                                                 , label_ranker=lr_flag\n",
    "                                                 , n_rollouts=30)   \n",
    "            \n",
    "            if preference_out is not None:\n",
    "                train_data.append(preference_out)\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    # process training data and learn a model\n",
    "    model = train_model(train_data)\n",
    "    \n",
    "    # derive a new policy using trained model\n",
    "    policy = Policy(act_space,model)\n",
    "    \n",
    "    # turn on inference from learnt policy\n",
    "    lr_flag = True\n",
    "    \n",
    "    # test the learned policy\n",
    "    env_test = gym.make('CartPole-v0')\n",
    "    reward_sum = 0\n",
    "    \n",
    "    for test in range(5):\n",
    "        obs = env_test.reset()\n",
    "        \n",
    "        for _ in range(1000):\n",
    "            action = policy.label_ranking_policy(obs)\n",
    "            observation, reward, done, info = env_test.step(action)\n",
    "            obs = observation\n",
    "            reward_sum += reward\n",
    "            if done: break\n",
    "        \n",
    "    env_test.close()\n",
    "    print(f'Average reward for 5 episodes: {reward_sum/5}', end='\\r')\n",
    "          \n",
    "    pbar.update(1)\n",
    "    iterr += 1\n",
    "        \n",
    "pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4ee9d5f7d371>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "### Test-run for learned policy ###\n",
    "\n",
    "# model path\n",
    "PATH = \"./models/cart_pole_pbpi_model.pt\"\n",
    "\n",
    "# Load\n",
    "model = Model(input_states.shape[1], len(act_space), [100,500,50], p=0.4)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "\n",
    "# create policy\n",
    "policy = Policy(act_space,model)\n",
    "\n",
    "\n",
    "# create environment and apply policy\n",
    "env = gym.make('CartPole-v0')\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    action = policy.label_ranking_policy(obs)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    obs = observation\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "# observe policy performance\n",
    "video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13920071,  0.19413263,  0.52746596,  0.86079929])"
      ]
     },
     "execution_count": 1070,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_space[0]\n",
    "partition_action_space(env,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09269119,  0.40730881,  0.90730881])"
      ]
     },
     "execution_count": 1071,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING DATASET PREP + MODEL + TRAINING [BREAKDOWN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>a_j</th>\n",
       "      <th>a_k</th>\n",
       "      <th>preference_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[0.009142906291385587, 0.3129712692496026]</td>\n",
       "      <td>-0.177859</td>\n",
       "      <td>0.822141</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[0.01540233167637764, 0.02318569560035527]</td>\n",
       "      <td>-0.177859</td>\n",
       "      <td>0.822141</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[0.015866045588384746, 0.3206881617194614]</td>\n",
       "      <td>-0.177859</td>\n",
       "      <td>0.822141</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        state       a_j       a_k  \\\n",
       "0  [0.009142906291385587, 0.3129712692496026] -0.177859  0.822141   \n",
       "1  [0.01540233167637764, 0.02318569560035527] -0.177859  0.822141   \n",
       "2  [0.015866045588384746, 0.3206881617194614] -0.177859  0.822141   \n",
       "\n",
       "   preference_label  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  "
      ]
     },
     "execution_count": 944,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a training dataframe\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a key for state\n",
    "train_df.loc[:, 'state_key'] = train_df.state.apply(lambda x: x[0].astype(str)+\"_\"+x[1].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all states which does not have any preferred action\n",
    "temp_df1 = train_df.groupby('state_key').preference_label.sum().reset_index()\n",
    "temp_df1 = temp_df1.loc[temp_df1.preference_label>0] # pick the states that have at least one prefered action\n",
    "train_df = train_df.merge(right = temp_df1.loc[:,'state_key']\n",
    "                          , right_on = 'state_key'\n",
    "                          , left_on = 'state_key'\n",
    "                          , how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a single column with the set of actions executed per state\n",
    "temp_df1 = train_df.groupby('state_key')['a_j'].unique().reset_index()\n",
    "temp_df2 = train_df.groupby('state_key')['a_k'].unique().reset_index()\n",
    "temp_df3 = temp_df1.merge(right=temp_df2\n",
    "                          , right_on = 'state_key'\n",
    "                          , left_on = 'state_key'\n",
    "                          , how = 'inner')\n",
    "temp_df3.loc[:,'unique_acts'] = temp_df3.apply(lambda row: set(list(row['a_j']) + list(row['a_k'])) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_key</th>\n",
       "      <th>a_j</th>\n",
       "      <th>a_k</th>\n",
       "      <th>unique_acts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.0024231983292896143_-0.28879385024693693</td>\n",
       "      <td>[-0.1778587942186142]</td>\n",
       "      <td>[0.8221412057813858]</td>\n",
       "      <td>{-0.1778587942186142, 0.8221412057813858}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.0078124528411575595_-1.108387582320789</td>\n",
       "      <td>[-0.1778587942186142]</td>\n",
       "      <td>[0.8221412057813858]</td>\n",
       "      <td>{-0.1778587942186142, 0.8221412057813858}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     state_key                    a_j  \\\n",
       "0  -0.0024231983292896143_-0.28879385024693693  [-0.1778587942186142]   \n",
       "1    -0.0078124528411575595_-1.108387582320789  [-0.1778587942186142]   \n",
       "\n",
       "                    a_k                                unique_acts  \n",
       "0  [0.8221412057813858]  {-0.1778587942186142, 0.8221412057813858}  \n",
       "1  [0.8221412057813858]  {-0.1778587942186142, 0.8221412057813858}  "
      ]
     },
     "execution_count": 948,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the unique-action column to train dataset\n",
    "train_df = train_df.merge(right = temp_df3.loc[:,['state_key', 'unique_acts']]\n",
    "              , right_on =  'state_key'\n",
    "              , left_on = 'state_key'\n",
    "              , how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 'prefered-action' value for each state\n",
    "train_df.loc[:,'prefered_action'] = train_df.apply(lambda row: row['a_j'] if row['preference_label'] == 1 else row['a_k']  ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>a_j</th>\n",
       "      <th>a_k</th>\n",
       "      <th>preference_label</th>\n",
       "      <th>state_key</th>\n",
       "      <th>unique_acts</th>\n",
       "      <th>prefered_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[0.022940822901645678, -0.25252027866834126]</td>\n",
       "      <td>-0.177859</td>\n",
       "      <td>0.822141</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022940822901645678_-0.25252027866834126</td>\n",
       "      <td>{-0.1778587942186142, 0.8221412057813858}</td>\n",
       "      <td>-0.177859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[0.017890417328278854, -0.5378798707657029]</td>\n",
       "      <td>-0.177859</td>\n",
       "      <td>0.822141</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017890417328278854_-0.5378798707657029</td>\n",
       "      <td>{-0.1778587942186142, 0.8221412057813858}</td>\n",
       "      <td>-0.177859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          state       a_j       a_k  \\\n",
       "0  [0.022940822901645678, -0.25252027866834126] -0.177859  0.822141   \n",
       "1   [0.017890417328278854, -0.5378798707657029] -0.177859  0.822141   \n",
       "\n",
       "   preference_label                                  state_key  \\\n",
       "0                 1  0.022940822901645678_-0.25252027866834126   \n",
       "1                 1   0.017890417328278854_-0.5378798707657029   \n",
       "\n",
       "                                 unique_acts  prefered_action  \n",
       "0  {-0.1778587942186142, 0.8221412057813858}        -0.177859  \n",
       "1  {-0.1778587942186142, 0.8221412057813858}        -0.177859  "
      ]
     },
     "execution_count": 951,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prefered_action</th>\n",
       "      <th>-0.177859</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_key</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>-0.0024231983292896143_-0.28879385024693693</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-0.0078124528411575595_-1.108387582320789</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-0.008199075334228353_-0.5822400438185005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-0.01984387621059836_-0.2921511934704404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-0.02568690008000717_-0.5910259816241711</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "prefered_action                              -0.177859\n",
       "state_key                                             \n",
       "-0.0024231983292896143_-0.28879385024693693          1\n",
       "-0.0078124528411575595_-1.108387582320789            1\n",
       "-0.008199075334228353_-0.5822400438185005            1\n",
       "-0.01984387621059836_-0.2921511934704404             1\n",
       "-0.02568690008000717_-0.5910259816241711             1"
      ]
     },
     "execution_count": 952,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of times each action is prefered at a state\n",
    "action_preference_counts = train_df.groupby('state_key').prefered_action.value_counts().unstack()\n",
    "action_preference_counts.replace(np.nan,0,inplace=True)\n",
    "action_preference_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert the action-preference-counts to a vector (to be used as training labels)\n",
    "action_preference_counts.loc[:, 'preference_label_vector'] = action_preference_counts.iloc[:,0:].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prefered_action</th>\n",
       "      <th>-0.1778587942186142</th>\n",
       "      <th>preference_label_vector</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>-0.0024231983292896143_-0.28879385024693693</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-0.0078124528411575595_-1.108387582320789</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-0.008199075334228353_-0.5822400438185005</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-0.01984387621059836_-0.2921511934704404</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>-0.02568690008000717_-0.5910259816241711</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "prefered_action                              -0.1778587942186142  \\\n",
       "state_key                                                          \n",
       "-0.0024231983292896143_-0.28879385024693693                    1   \n",
       "-0.0078124528411575595_-1.108387582320789                      1   \n",
       "-0.008199075334228353_-0.5822400438185005                      1   \n",
       "-0.01984387621059836_-0.2921511934704404                       1   \n",
       "-0.02568690008000717_-0.5910259816241711                       1   \n",
       "\n",
       "prefered_action                              preference_label_vector  \n",
       "state_key                                                             \n",
       "-0.0024231983292896143_-0.28879385024693693                        1  \n",
       "-0.0078124528411575595_-1.108387582320789                          1  \n",
       "-0.008199075334228353_-0.5822400438185005                          1  \n",
       "-0.01984387621059836_-0.2921511934704404                           1  \n",
       "-0.02568690008000717_-0.5910259816241711                           1  "
      ]
     },
     "execution_count": 954,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_preference_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add preference label vector to training dataset\n",
    "train_df = train_df.merge(right = action_preference_counts.loc[:,['preference_label_vector']]\n",
    "                          , right_index= True\n",
    "                          , left_on = 'state_key'\n",
    "                          , how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>a_j</th>\n",
       "      <th>a_k</th>\n",
       "      <th>preference_label</th>\n",
       "      <th>state_key</th>\n",
       "      <th>unique_acts</th>\n",
       "      <th>prefered_action</th>\n",
       "      <th>preference_label_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[0.022940822901645678, -0.25252027866834126]</td>\n",
       "      <td>-0.177859</td>\n",
       "      <td>0.822141</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022940822901645678_-0.25252027866834126</td>\n",
       "      <td>{-0.1778587942186142, 0.8221412057813858}</td>\n",
       "      <td>-0.177859</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[0.017890417328278854, -0.5378798707657029]</td>\n",
       "      <td>-0.177859</td>\n",
       "      <td>0.822141</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017890417328278854_-0.5378798707657029</td>\n",
       "      <td>{-0.1778587942186142, 0.8221412057813858}</td>\n",
       "      <td>-0.177859</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          state       a_j       a_k  \\\n",
       "0  [0.022940822901645678, -0.25252027866834126] -0.177859  0.822141   \n",
       "1   [0.017890417328278854, -0.5378798707657029] -0.177859  0.822141   \n",
       "\n",
       "   preference_label                                  state_key  \\\n",
       "0                 1  0.022940822901645678_-0.25252027866834126   \n",
       "1                 1   0.017890417328278854_-0.5378798707657029   \n",
       "\n",
       "                                 unique_acts  prefered_action  \\\n",
       "0  {-0.1778587942186142, 0.8221412057813858}        -0.177859   \n",
       "1  {-0.1778587942186142, 0.8221412057813858}        -0.177859   \n",
       "\n",
       "   preference_label_vector  \n",
       "0                        1  \n",
       "1                        1  "
      ]
     },
     "execution_count": 956,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>state_key</th>\n",
       "      <th>unique_acts</th>\n",
       "      <th>preference_label_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[0.022940822901645678, -0.25252027866834126]</td>\n",
       "      <td>0.022940822901645678_-0.25252027866834126</td>\n",
       "      <td>{-0.1778587942186142, 0.8221412057813858}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[0.017890417328278854, -0.5378798707657029]</td>\n",
       "      <td>0.017890417328278854_-0.5378798707657029</td>\n",
       "      <td>{-0.1778587942186142, 0.8221412057813858}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[0.007132819912964796, -0.824872468474162]</td>\n",
       "      <td>0.007132819912964796_-0.824872468474162</td>\n",
       "      <td>{-0.1778587942186142, 0.8221412057813858}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          state  \\\n",
       "0  [0.022940822901645678, -0.25252027866834126]   \n",
       "1   [0.017890417328278854, -0.5378798707657029]   \n",
       "2    [0.007132819912964796, -0.824872468474162]   \n",
       "\n",
       "                                   state_key  \\\n",
       "0  0.022940822901645678_-0.25252027866834126   \n",
       "1   0.017890417328278854_-0.5378798707657029   \n",
       "2    0.007132819912964796_-0.824872468474162   \n",
       "\n",
       "                                 unique_acts  preference_label_vector  \n",
       "0  {-0.1778587942186142, 0.8221412057813858}                        1  \n",
       "1  {-0.1778587942186142, 0.8221412057813858}                        1  \n",
       "2  {-0.1778587942186142, 0.8221412057813858}                        1  "
      ]
     },
     "execution_count": 957,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the reduced training dataset (drop unnecessary columns + duplicate rows: which have data for same state)\n",
    "train_df_reduced = train_df.loc[:,['state', 'state_key', 'unique_acts', 'preference_label_vector']]\n",
    "\n",
    "train_df_reduced.drop_duplicates(subset=['state_key'],inplace=True)\n",
    "train_df_reduced.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above dataset has - state + list of actions + # preferences made for each action (at state) --> each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_labels_temp = np.array(train_df_reduced.preference_label_vector.tolist())\n",
    "\n",
    "# to handle 1-D output label vector\n",
    "if len(output_labels_temp.shape) == 1:\n",
    "    output_labels_normalized = output_labels_temp\n",
    "else:\n",
    "    row_sums = output_labels_temp.sum(axis=1)\n",
    "    output_labels_normalized = output_labels_temp / row_sums[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training dataset\n",
    "input_states  = torch.from_numpy(np.array(train_df_reduced.state.apply(lambda x: x.astype(float)).tolist()))\n",
    "output_labels = torch.from_numpy(output_labels_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensor dataset & data loader\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(input_states , output_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.1020,  0.0784],\n",
       "         [-0.1270, -0.9492],\n",
       "         [-0.1146, -0.6227]], dtype=torch.float64),\n",
       " tensor([1, 1, 1], dtype=torch.int32)]"
      ]
     },
     "execution_count": 1033,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define data loader\n",
    "batch_size = 3\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, drop_last=True)\n",
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, input_state_len, output_label_len, layers, p=0.4):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.batch_norm_num = nn.BatchNorm1d(input_state_len)\n",
    "\n",
    "        all_layers = []\n",
    "        input_size = input_state_len\n",
    "\n",
    "        # create layers\n",
    "        for layer_dim in layers:\n",
    "            all_layers.append(nn.Linear(input_size, layer_dim))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(layer_dim))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = layer_dim\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_label_len))\n",
    "\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, state_vec):\n",
    "        x = self.batch_norm_num(state_vec)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = Model(input_states.shape[1], output_labels.shape[1], [100,500,50], p=0.4)\n",
    "# model = Model(input_states.shape[1], 1 if len(output_labels.shape)== 1 else output_labels.shape[1], [100,500,50], p=0.4) # for 1-D targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (batch_norm_num): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=500, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=500, out_features=50, bias=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.4, inplace=False)\n",
       "    (12): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1037,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr = 1e-1)\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for xb,yb in train_dl:\n",
    "    print(yb.reshape(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store losses\n",
    "aggregated_losses = []\n",
    "\n",
    "# Define a utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # Generate predictions\n",
    "            pred = model(xb.float())\n",
    "            #loss = loss_fn(pred, yb.float())            \n",
    "            loss = loss_fn(pred, yb.reshape(3,1).float()) \n",
    "            \n",
    "            # Perform gradient descent\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        #aggregated_losses.append(loss_fn(model(input_states.float()), output_labels.float()).detach().numpy())\n",
    "        \n",
    "        if epoch%25 == 0:\n",
    "                print(f'epoch: {epoch:3} loss: {loss.item():10.8f}')\n",
    "            \n",
    "    print('Training loss: ', loss_fn(model(input_states.float()), output_labels.float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 1052,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dl))\n",
    "pred = model(xb.float())\n",
    "#loss_fn(pred, yb.float())  \n",
    "yb.reshape(3,1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 loss:        nan\n",
      "epoch:  25 loss:        nan\n",
      "epoch:  50 loss:        nan\n",
      "epoch:  75 loss:        nan\n",
      "epoch: 100 loss:        nan\n",
      "epoch: 125 loss:        nan\n",
      "epoch: 150 loss:        nan\n",
      "epoch: 175 loss:        nan\n",
      "epoch: 200 loss:        nan\n",
      "epoch: 225 loss:        nan\n",
      "epoch: 250 loss:        nan\n",
      "epoch: 275 loss:        nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Using a target size (torch.Size([60])) that is different to the input size (torch.Size([60, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  tensor(nan, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "epochs= 300\n",
    "fit(epochs, model, loss_fn, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), aggregated_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Ranking Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "\n",
    "state_obs = observation[[2,3]] # only select angel and angular velo. of pendulum from state vector\n",
    "state_obs = state_obs.reshape(-1,state_obs.shape[0]) # reshape to be a 2D array\n",
    "state_obs = torch.from_numpy(state_obs) # convert to a tensor\n",
    "\n",
    "# make the prediction for actions\n",
    "with torch.no_grad():\n",
    "    preds = model(state_obs.float()) \n",
    "    \n",
    "# rank the indexes of actions (from highest ranked/preferred action to lowest)\n",
    "ranked_action_idx = (-rd(preds.detach().numpy())).argsort()[:preds.shape[1]]\n",
    "\n",
    "# return the action value\n",
    "remain_probs = .1/len(ranked_action_idx[2:])\n",
    "n_remain_actions = ranked_action_idx.shape[0]-2\n",
    "\n",
    "# - select first two (highest preferred actions) 80% and 10% of the time\n",
    "# - select one of the remaining actions 10% time\n",
    "action = np.random.choice(ranked_action_idx,1 , p=[0.8, 0.1] + list(np.repeat(remain_probs,n_remain_actions)))[0]\n",
    "\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### METHOD 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 6] The handle is invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1091-e3948c38d567>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CartPole-v0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./gym-results\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         )\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_frames_per_sec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoder_version'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, output_path, frame_shape, frames_per_sec, output_frames_per_sec)\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetsid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcapture_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    662\u001b[0m                  pass_fds=(), *, encoding=None, errors=None, text=None):\n\u001b[0;32m    663\u001b[0m         \u001b[1;34m\"\"\"Create new Popen instance.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m         \u001b[1;31m# Held while anything is calling waitpid before returncode has been\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m# updated to prevent clobbering returncode if wait() or poll() are\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_cleanup\u001b[1;34m()\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minst\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_active\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_deadstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_internal_poll\u001b[1;34m(self, _deadstate, _WaitForSingleObject, _WAIT_OBJECT_0, _GetExitCodeProcess)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m_WaitForSingleObject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_WAIT_OBJECT_0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_GetExitCodeProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 6] The handle is invalid"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### METHOD 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1077-415a9fafd9f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnb_episodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnb_timesteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# only call this once\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# iterate over the episodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2681\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2682\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[1;32m-> 2683\u001b[1;33m         None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2684\u001b[0m     \u001b[0msci\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2685\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1599\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1601\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m                 \u001b[1;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[1;32m--> 369\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m                 \u001b[1;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[1;32m--> 369\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5669\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5671\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5672\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5673\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    683\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[0;32m    684\u001b[0m             raise TypeError(\"Image data of dtype {} cannot be converted to \"\n\u001b[1;32m--> 685\u001b[1;33m                             \"float\".format(self._A.dtype))\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m         if not (self._A.ndim == 2\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_episodes = 20\n",
    "nb_timesteps = 100\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "\n",
    "for episode in range(nb_episodes):  # iterate over the episodes\n",
    "    state = env.reset()             # initialise the environment\n",
    "    rewards = []\n",
    "    \n",
    "    for t in range(nb_timesteps):    # iterate over time steps\n",
    "        #env.render()                 # display the environment\n",
    "        img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, info = env.step(0)  # implement the action chosen by the policy\n",
    "        rewards.append(reward)      # add 1 to the rewards list\n",
    "        \n",
    "        if done: # the episode ends either if the pole is > 15 deg from vertical or the cart move by > 2.4 unit from the centre\n",
    "            cumulative_reward = sum(rewards)\n",
    "            print(\"episode {} finished after {} timesteps. Total reward: {}\".format(episode, t+1, cumulative_reward))  \n",
    "            break\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OTHER USEFUL-INFO:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- useful links\n",
    "    - how to initialize to a custom state: https://stackoverflow.com/questions/57263759/how-can-i-start-the-environment-from-a-custom-initial-state-for-mountain-car"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
