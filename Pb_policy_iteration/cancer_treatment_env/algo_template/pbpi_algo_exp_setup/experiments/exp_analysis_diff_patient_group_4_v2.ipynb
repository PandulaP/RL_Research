{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd02d430aa5654386b74de248f177970fbace922f61e00ea2a28d891336b8dbee77",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# EXPERIMENT - STATE-GROUP-4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# State Groups:\n",
    "\n",
    "- `group_1`: Patients with low toxicity and low tumor size  - W_0 ~ N(0.5, 0.16**2), T_0 ~ N(0.5, 0.16**2)\n",
    "- `group_2`: Patients with high toxicity and low tumor size - W_0 ~ N(1.5, 0.16**2), T_0 ~ N(0.5, 0.16**2)\n",
    "- `group_3`: Patients with low initial toxicity and high tumor size - W_0 ~ N(0.5, 0.16**2), T_0 ~ N(1.5, 0.16**2)\n",
    "- `group_4`: Patients with high toxicity and high tumor size - W_0 ~ N(1.5, 0.16**2), T_0 ~ N(1.5, 0.16**2)\n",
    "-  `None`  : Patients with toxicity and tumor size sample from - W_0 ~ U(0,2), T_0 ~  U(0,2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tqdm\n",
    "import _set_path\n",
    "from pbpi.algo_core.training import evaluations_per_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGO_TYPE = {'original': {'name': 'original', 'exploration': False}\n",
    "            ,'modified': {'name': 'modified', 'exploration': True} }"
   ]
  },
  {
   "source": [
    "---\n",
    "## Using `tumor size + toxicity` to generate preferences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Original algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "################## INPUTS ##################\n",
    "\n",
    "# Configs to test\n",
    "N_STATES = 1000\n",
    "\n",
    "configs = { 'CONFIG_NO': 3\n",
    "          , 'S': [N_STATES]\n",
    "          , 'Actions' : [6]\n",
    "          , 'Roll-outs': [10]\n",
    "          , 'Significance' : [0.1]\n",
    "          , 'init_state_group': 'group_4'\n",
    "          , 'init_state_tag': 'g4'\n",
    "          }\n",
    "\n",
    "algorithm = ALGO_TYPE['original']\n",
    "\n",
    "############################################\n",
    "############################################\n",
    "\n",
    "# Algorithm configs\n",
    "ALGO_NAME = algorithm['name']\n",
    "EXPLORE_LOGIC = algorithm['exploration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Runs:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "State generation seed is 3\n",
      "\n",
      "\n",
      "Currently evaluated configs:\n",
      " Samples: 1000 | Actions: 6 | Roll-outs: 10 | Significance: 0.1\n",
      "No training data collected!\n",
      "No training data collected!\n",
      "No training data collected!\n",
      "Runs:   0%|          | 0/1 [37:27<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d976fc6c3fc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msig_lvl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Significance'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             run_results = evaluations_per_config(s_size                     = sample_size\n\u001b[0m\u001b[0;32m     15\u001b[0m                                                 \u001b[1;33m,\u001b[0m \u001b[0minit_state_group\u001b[0m          \u001b[1;33m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'init_state_group'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Github\\RL_Research\\Pb_policy_iteration\\cancer_treatment_env\\algo_template\\pbpi_algo_exp_setup\\pbpi\\algo_core\\training.py\u001b[0m in \u001b[0;36mevaluations_per_config\u001b[1;34m(s_size, n_actions, max_n_rollouts, sig_lvl, runs_per_config, max_policy_iter_per_run, eval_runs_per_state, treatment_length_train, treatment_length_eval, off_policy_explr, env_name, init_state_group, show_experiment_run_eval_summary_plot, rollout_tracking, dataset_tracking, train_plot_tracking, eval_summary_tracking, policy_behaviour_tracking, set_seed, init_state_tag, use_toxi_n_tumor_for_pref)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                     \u001b[1;31m# generate preference data & executed num. of actions in each action pair evaluation step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                     preference_out, actions_per_pair = evaluate_preference(starting_state = state\n\u001b[0m\u001b[0;32m    176\u001b[0m                                                                             \u001b[1;33m,\u001b[0m \u001b[0maction_1\u001b[0m       \u001b[1;33m=\u001b[0m \u001b[0maction_pair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                                                                             \u001b[1;33m,\u001b[0m \u001b[0maction_2\u001b[0m       \u001b[1;33m=\u001b[0m \u001b[0maction_pair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Github\\RL_Research\\Pb_policy_iteration\\cancer_treatment_env\\algo_template\\pbpi_algo_exp_setup\\pbpi\\algo_core\\pbpi_algo_base.py\u001b[0m in \u001b[0;36mevaluate_preference\u001b[1;34m(starting_state, action_1, action_2, policy_in, environment_name, n_rollouts, max_rollout_len, label_ranker, modified_algo, p_sig, tracking, use_toxi_n_tsize)\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                     \u001b[1;31m# Sample next state using a random policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m                     \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_death\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m                     \u001b[0maction_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# Increase the action count by 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Github\\RL_Research\\Pb_policy_iteration\\cancer_treatment_env\\algo_template\\pbpi_algo_exp_setup\\pbpi\\algo_core\\training.py\u001b[0m in \u001b[0;36mrandom_action\u001b[1;34m(environment, seed)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m#environment.seed(seed)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\spaces\\box.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m             size=upp_bounded[upp_bounded].shape) + self.high[upp_bounded]\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         sample[bounded] = self.np_random.uniform(low=self.low[bounded],\n\u001b[0m\u001b[0;32m    118\u001b[0m                                             \u001b[0mhigh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhigh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbounded\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                                             size=bounded[bounded].shape)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Experiment execution\n",
    "agg_results = []\n",
    "\n",
    "eval_count = len(configs['S'])*len(configs['Actions'])*len(configs['Roll-outs'])*len(configs['Significance'])\n",
    "\n",
    "pbar_evals = tqdm.tqdm(total=eval_count, desc=\"Evaluations\")\n",
    "\n",
    "for sample_size in configs['S']:\n",
    "        \n",
    "    for rollout_max in configs['Roll-outs']:\n",
    "\n",
    "        for sig_lvl in configs['Significance']:\n",
    "\n",
    "            run_results = evaluations_per_config(s_size                     = sample_size\n",
    "                                                , init_state_group          = configs['init_state_group']\n",
    "\n",
    "                                                , n_actions                 = configs['Actions'][0]\n",
    "                                                , max_n_rollouts            = rollout_max\n",
    "                                                , sig_lvl                   = sig_lvl\n",
    "\n",
    "                                                , treatment_length_train    = 9\n",
    "                                                , treatment_length_eval     = 9\n",
    "                                                \n",
    "                                                , max_policy_iter_per_run   = 10 # Maximum number of policy iterations per experiment\n",
    "                                                , runs_per_config           = 1  # Number of experiments per one parameter config\n",
    "\n",
    "                                                , eval_runs_per_state       = 1  # Episodes to generate from each init. state during evaluation phrase\n",
    "                                                \n",
    "                                                , off_policy_explr          = EXPLORE_LOGIC # What algorithm to use\n",
    "\n",
    "                                                , rollout_tracking          = False # Show rollout info.\n",
    "                                                , dataset_tracking          = False # Show train dataset\n",
    "\n",
    "                                                , train_plot_tracking       = False # Show model training plot\n",
    "                                                , eval_summary_tracking     = True  # Show a policy performance summary of evaluation runs\n",
    "                                                , policy_behaviour_tracking = False # Show/store policy action selections vs. pendulum angle plot\n",
    "\n",
    "                                                , show_experiment_run_eval_summary_plot = False # Show death-rate/tumor-size+toxicity vs. action no. plot of exp. run\n",
    "\n",
    "                                                , use_toxi_n_tumor_for_pref = True # Generate preferences using sum(max-toxi, tumor-size) of rollouts\n",
    "                                                , set_seed                  = 3\n",
    "                                                , init_state_tag            = configs['init_state_tag']\n",
    "                                                )\n",
    "\n",
    "            agg_results.append(run_results)\n",
    "\n",
    "            pbar_evals.update(1)\n",
    "                \n",
    "pbar_evals.close()"
   ]
  },
  {
   "source": [
    "### Modified algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "################## INPUTS ##################\n",
    "\n",
    "# Configs to test\n",
    "N_STATES = 1000\n",
    "\n",
    "configs = { 'CONFIG_NO': 4\n",
    "          , 'S': [N_STATES]\n",
    "          , 'Actions' : [6]\n",
    "          , 'Roll-outs': [10]\n",
    "          , 'Significance' : [0.1]\n",
    "          , 'init_state_group': 'group_4'\n",
    "          , 'init_state_tag': 'g4'\n",
    "          }\n",
    "\n",
    "algorithm = ALGO_TYPE['modified']\n",
    "\n",
    "############################################\n",
    "############################################\n",
    "\n",
    "# Algorithm configs\n",
    "ALGO_NAME = algorithm['name']\n",
    "EXPLORE_LOGIC = algorithm['exploration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment execution\n",
    "agg_results = []\n",
    "\n",
    "eval_count = len(configs['S'])*len(configs['Actions'])*len(configs['Roll-outs'])*len(configs['Significance'])\n",
    "\n",
    "pbar_evals = tqdm.tqdm(total=eval_count, desc=\"Evaluations\")\n",
    "\n",
    "for sample_size in configs['S']:\n",
    "        \n",
    "    for rollout_max in configs['Roll-outs']:\n",
    "\n",
    "        for sig_lvl in configs['Significance']:\n",
    "\n",
    "            run_results = evaluations_per_config(s_size                     = sample_size\n",
    "                                                , init_state_group          = configs['init_state_group']\n",
    "\n",
    "                                                , n_actions                 = configs['Actions'][0]\n",
    "                                                , max_n_rollouts            = rollout_max\n",
    "                                                , sig_lvl                   = sig_lvl\n",
    "\n",
    "                                                , treatment_length_train    = 9\n",
    "                                                , treatment_length_eval     = 9\n",
    "                                                \n",
    "                                                , max_policy_iter_per_run   = 10 # Maximum number of policy iterations per experiment\n",
    "                                                , runs_per_config           = 1  # Number of experiments per one parameter config\n",
    "\n",
    "                                                , eval_runs_per_state       = 1  # Episodes to generate from each init. state during evaluation phrase\n",
    "                                                \n",
    "                                                , off_policy_explr          = EXPLORE_LOGIC # What algorithm to use\n",
    "\n",
    "                                                , rollout_tracking          = False # Show rollout info.\n",
    "                                                , dataset_tracking          = False # Show train dataset\n",
    "\n",
    "                                                , train_plot_tracking       = False # Show model training plot\n",
    "                                                , eval_summary_tracking     = True  # Show a policy performance summary of evaluation runs\n",
    "                                                , policy_behaviour_tracking = False # Show/store policy action selections vs. pendulum angle plot\n",
    "\n",
    "                                                , show_experiment_run_eval_summary_plot = False # Show death-rate/tumor-size+toxicity vs. action no. plot of exp. run\n",
    "\n",
    "                                                , use_toxi_n_tumor_for_pref = True # Generate preferences using sum(max-toxi, tumor-size) of rollouts\n",
    "                                                , set_seed                  = 3\n",
    "                                                , init_state_tag            = configs['init_state_tag']\n",
    "                                                )\n",
    "\n",
    "            agg_results.append(run_results)\n",
    "\n",
    "            pbar_evals.update(1)\n",
    "                \n",
    "pbar_evals.close()"
   ]
  }
 ]
}