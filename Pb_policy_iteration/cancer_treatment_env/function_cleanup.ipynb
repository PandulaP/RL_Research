{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "2d430aa5654386b74de248f177970fbace922f61e00ea2a28d891336b8dbee77"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import chemo_simulation # custom cart-pole environment\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial patient set\n",
    "tumor_size_init, toxicity_init = np.random.uniform(low=0, high=2, size = (2, 1000))\n",
    "init_patients = [(t_size, toxicity) for t_size, toxicity in zip(tumor_size_init,toxicity_init)]\n",
    "\n",
    "# Dosages to provide to patient\n",
    "dosage_levels = np.array([0.1 , .3 , .5, .7, .1])\n",
    "\n",
    "# Random policy to use\n",
    "def random_policy(env):\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "source": [
    "#### Evaluate preferences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.descriptivestats import sign_test\n",
    "\n",
    "def evaluate_preference(starting_state # starting state of roll-outs\n",
    "                        , action_1     # first action to execute at the starting-state\n",
    "                        , action_2     # second action to execute at the starting state\n",
    "                        , policy_in    # policy to folow\n",
    "                        , environment_name = 'ChemoSimulation-v0'   # name of the environment\n",
    "                        #, discount_fac = 1        # discounting factor\n",
    "                        , n_rollouts = 10         # number of roll-outs to generate per action\n",
    "                        , max_rollout_len = 7     # maximum length of a roll-out\n",
    "                        , label_ranker = False    # whether to use the label-ranking model or not\n",
    "                        , modified_algo = False   # Whether evaluations run for modified algorithm or not\n",
    "                        , p_sig = 0.1            # p-value to use for t-test (to compare returns of roll-outs)\n",
    "                        , tracking = False\n",
    "                        ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "    \n",
    "        - Roll-outs are generated at each state in the initial state set by starting from the given input action \n",
    "          and following the given policy afterwards. \n",
    "        - Returns of the roll-outs are used to generate preferences for the input action pair.\n",
    "        - Generated preferences are returned to be create a training dataset to learn the LabelRanker model.    \n",
    "    \"\"\"\n",
    "    \n",
    "    # initializing variables\n",
    "    policy = policy_in          \n",
    "    n_rollouts = n_rollouts     \n",
    "    gamma = discount_fac    \n",
    "    s_init = starting_state\n",
    "    max_traj_len = max_rollout_len \n",
    "        \n",
    "    # we store the num. actions executed within the evaluation process (to measure complexity)\n",
    "    action_count = 0 \n",
    "        \n",
    "    # Dictionary to store input action values\n",
    "    actions = { 'one' : action_1    \n",
    "              , 'two' : action_2}    \n",
    "\n",
    "    # Dictionary to store tumor sizes at the end of roll-outs\n",
    "    t_mass = { 'one' : [None]*n_rollouts \n",
    "              , 'two' : [None]*n_rollouts}\n",
    "\n",
    "    # Dictionary to store max toxicity values of roll-outs\n",
    "    toxicity = { 'one' : [None]*n_rollouts \n",
    "              , 'two'  : [None]*n_rollouts}  \n",
    "\n",
    "    # Dictionary to store the probability of death at the end of roll-outs\n",
    "    prob_death = { 'one' : [None]*n_rollouts \n",
    "                  , 'two'  : [None]*n_rollouts}  \n",
    "\n",
    "    # Dictionary to rollout-preferences for each action\n",
    "    accu_preferences = {'one': 0, 'two': 0}  \n",
    "\n",
    "    # List to store sign-test data of rollouts\n",
    "    sign_test_vals = [None] * n_rollouts\n",
    "    \n",
    "    # Generate the defined number of roll-outs for selected action\n",
    "    for rollout in range(n_rollouts):\n",
    "\n",
    "        # Select each starting action of the input actions to generate a roll-out\n",
    "        for action_key, action_value in actions.items():\n",
    "\n",
    "            # Create an environment object and set the starting state to the input (initial) state\n",
    "            env = gym.make(environment_name)\n",
    "            env.reset(init_state=s_init) \n",
    "\n",
    "            # Apply the action\n",
    "            observation, reward, done, _ = env.step(np.array([action_value]))\n",
    "            \n",
    "            # Define the history variable to store the last observed state\n",
    "            hist = observation \n",
    "            \n",
    "            # Variables to store the last tumor-size, max-toxicity, and the probab. of death of a rollout\n",
    "            max_toxicity  = -1_000\n",
    "            last_t_mass   = None\n",
    "            last_p_death  = None\n",
    "\n",
    "            # Follow the given policy to generate a roll-out trajectory \n",
    "            traj_len = 1\n",
    "            while traj_len < max_traj_len and not done: \n",
    "                                \n",
    "                # Sample next state using the label-ranking model (if TRUE)\n",
    "                if label_ranker: \n",
    "                    observation, reward, done, p_death = env.step(policy.label_ranking_policy(hist))\n",
    "\n",
    "                    # Replace current history with the observed state\n",
    "                    hist = observation\n",
    "                    action_count+=1   # Increase the action count by 1\n",
    "                \n",
    "                else:\n",
    "                    # Sample next state using a random policy \n",
    "                    observation, reward, done, p_death = env.step(policy(env))\n",
    "                    action_count += 1 # Increase the action count by 1\n",
    "\n",
    "                # Increment the trajectory length count by 1\n",
    "                traj_len += 1\n",
    "\n",
    "                # Update the placeholder variables with latest values\n",
    "                last_t_mass  = observation[0]\n",
    "                max_toxicity = max(max_toxicity, observation[1])\n",
    "                last_p_death = p_death\n",
    "\n",
    "            # Store the last observed rollout information\n",
    "            t_mass[action_key][rollout]     = last_t_mass\n",
    "            toxicity[action_key][rollout]   = max_toxicity\n",
    "            prob_death[action_key][rollout] = last_p_death\n",
    "\n",
    "            # close the environment after creating all roll-outs for a specific starting action\n",
    "            env.close()\n",
    "            del env\n",
    "\n",
    "\n",
    "        # Generate preference relation information for the rollout\n",
    "        if (np.round(prob_death['one'][rollout],2)  >= 1.0) and (np.round(prob_death['two'][rollout],2) >= 1.0):\n",
    "            continue\n",
    "\n",
    "        elif (np.round(prob_death['one'][rollout],2)  < 1.0) and (np.round(prob_death['two'][rollout],2) >= 1.0):\n",
    "            # Patient survives for the trajectory starting from action 'one'\n",
    "            accu_preferences['one'] += 1\n",
    "            sign_test_vals[rollout] = +1\n",
    "\n",
    "        elif (np.round(prob_death['one'][rollout],2) >= 1.0) and (np.round(prob_death['two'][rollout],2) < 1.0):\n",
    "            # Patient survives for the trajectory starting from action 'two'\n",
    "            accu_preferences['two'] += 1\n",
    "            sign_test_vals[rollout] = -1\n",
    "\n",
    "\n",
    "        elif (np.round(prob_death['one'][rollout],2) < 1.0) and (np.round(prob_death['one'][rollout],2) < 1.0):\n",
    "            # Patient survives for both trajectories starting from both actions\n",
    "            \n",
    "            if (np.round(toxicity['one'][rollout],4) <= np.round(toxicity['two'][rollout],4)) and \\\n",
    "               (np.round(t_mass['one'][rollout],4)  <= np.round(t_mass['two'][rollout],4)):\n",
    "               # Max toxicity and tumor size of the patient starting from action 'one' is smaller\n",
    "               accu_preferences['one'] += 1\n",
    "               sign_test_vals[rollout] = +1\n",
    "\n",
    "            if (np.round(toxicity['one'][rollout],4) >= np.round(toxicity['two'][rollout],4)) and \\\n",
    "                (np.round(t_mass['one'][rollout],4) >= np.round(t_mass['two'][rollout],4)):\n",
    "                # Max toxicity and tumor size of the patient starting from action 'two' is smaller\n",
    "                accu_preferences['two'] += 1\n",
    "\n",
    "                # In case both max. toxicity and tumor size are equal for both action trajectories\n",
    "                if sign_test_vals[rollout] == +1:\n",
    "                    sign_test_vals[rollout] = 0\n",
    "                else:\n",
    "                    sign_test_vals[rollout] = -1\n",
    "        \n",
    "        else:\n",
    "            # No preference is generated for the rollout\n",
    "            accu_preferences['one'] += 0\n",
    "            accu_preferences['two'] += 0\n",
    "            sign_test_vals[rollout] = 0\n",
    "\n",
    "\n",
    "    # Clean-up sign-test data after removing 'None' entries\n",
    "    sign_test_vals = [val for val in sign_test_vals if val is not None]\n",
    "    \n",
    "    # Run sign-test\n",
    "    m, p_val = sign_test(sign_test_vals)\n",
    "\n",
    "    # print('mass', t_mass, '\\n\\ntoxicity',toxicity, '\\n\\np-death',prob_death)\n",
    "    # print('Action 1 preferences:' , accu_preferences['one']\n",
    "    #         , 'Action 2 preferences:', accu_preferences['two']\n",
    "    #         , 'sign_test', sign_test_vals, 'p-val', p_val, 'm', m)\n",
    "\n",
    "    # track output\n",
    "    if tracking:\n",
    "        print(f\"state: {s_init} | a_j(R): {accu_preferences['one']} | a_k(R): {accu_preferences['two']} | sig: {'Yes' if (p_val <= p_sig) else '--'}\")\n",
    "    \n",
    "    # return preference information\n",
    "    if (m > 0) and (p_val <= p_sig):\n",
    "        return {'state': s_init\n",
    "               , 'a_j' : actions['one']\n",
    "               , 'a_k' : actions['two']\n",
    "               , 'preference_label' : 1}, action_count\n",
    "    \n",
    "    elif(m < 0) and (p_val <= p_sig):\n",
    "        return {'state': s_init\n",
    "               , 'a_j' : actions['one']\n",
    "               , 'a_k' : actions['two']\n",
    "               , 'preference_label' : 0}, action_count\n",
    "    \n",
    "    # return NaN if avg. returns are not significantly different from each other OR are equal\n",
    "    else: \n",
    "        return {'state': np.nan\n",
    "               , 'a_j' : np.nan\n",
    "               , 'a_k' : np.nan\n",
    "               , 'preference_label' : np.nan}, action_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0.9121673849763303, 0.8767353694253202)\nstate: (0.9121673849763303, 0.8767353694253202) | a_j(R): 0 | a_k(R): 5 | sig: Yes\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'state': (0.9121673849763303, 0.8767353694253202),\n",
       "  'a_j': 0.1,\n",
       "  'a_k': 0.3,\n",
       "  'preference_label': 0},\n",
       " 300)"
      ]
     },
     "metadata": {},
     "execution_count": 560
    }
   ],
   "source": [
    "init_state = init_patients[np.random.randint(1000)]\n",
    "print(init_state)\n",
    "evaluate_preference(starting_state=init_state\n",
    "                    , action_1=dosage_levels[0]\n",
    "                    , action_2=dosage_levels[1]\n",
    "                    , policy_in=random_policy\n",
    "                    , n_rollouts = 25\n",
    "                    , tracking=True)"
   ]
  },
  {
   "source": [
    "#### Model training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "### LabelRanker Model training process ###\n",
    "\n",
    "def train_model(train_data                  # collection of all preference data\n",
    "                , action_space              # action space of the task\n",
    "                , model_name:str            # name for the model (to store)\n",
    "                , batch_s = 4               # batch size to train the NN model\n",
    "                , mod_layers = [10]         # model configuration\n",
    "                , n_epochs = 1000           # num. of epochs to train the model\n",
    "                , l_rate = 0.01             # learning rate for the optimization process  \n",
    "                , retrain_model = False     # whether to retrain the same model or not\n",
    "                , policy_iterr_count = None # policy iteration count\n",
    "                , show_train_plot = False   # flag to display the 'training-loss vs. epoch' plot\n",
    "                , show_dataset = False):    # flag to display the training dataset\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "    \n",
    "        - This function process all preference data to construct a training dataset for the LabelRanker model.\n",
    "        - One training sample takes the form:\n",
    "            X: [state-value (2-D)]\n",
    "            Y: [(normalized) ranking of actions (n-D)], where 'n' is the number of actions in the action space.\n",
    "        - For a given (2-D) state input, the (trained) model, i.e., LabelRanker, predicts the rank of \n",
    "           all possible actions at the input state \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    ### creating the training dataset ###\n",
    "        \n",
    "    # convert training data input to a dataframe | \n",
    "    # remove the rows that have NaN, i.e.,preference evaluations without any action preference\n",
    "    train_df = pd.DataFrame(train_data).dropna()\n",
    "\n",
    "    # create a key for each state in the dataset\n",
    "    # (only select the 'pendulum-velocity & pendulum-angle)\n",
    "    #train_df.loc[:, 'state_key'] = train_df.state.apply(lambda x: x[2].astype(str)+\"_\"+x[3].astype(str))\n",
    "    #train_df.loc[:, 'state_key'] = train_df.state.apply(lambda x: round(x[2].reshape(-1)[0],6).astype(str)+\"_\"+round(x[3].reshape(-1)[0],6).astype(str))\n",
    "    train_df.loc[:, 'state_key'] = train_df.state.apply(lambda x: x[2].reshape(-1)[0].astype(str)+\"_\"+x[3].reshape(-1)[0].astype(str))\n",
    "\n",
    "    \n",
    "    # ******************************** # EXPERIMENTAL STEP START\n",
    "    # create a full state key (state+action preference)\n",
    "    #train_df.loc[:, 'state_action_key'] = train_df.state.apply(lambda x: round(x[2],6).astype(str)+\"_\"+round(x[3],6).astype(str)) +\"_\"+ train_df.a_j.apply(lambda x: x[0][0].astype(str))+\"_\"+ train_df.a_k.apply(lambda x: x[0][0].astype(str)) \n",
    "\n",
    "    \n",
    "    # drop duplicates (if one training-set maintained) : only keep the first learned preference\n",
    "    #train_df.drop_duplicates(subset=['state_key'], keep='first', inplace=True)\n",
    "    #train_df.drop_duplicates(subset=['state_action_key'], keep='first', inplace=True)\n",
    "    \n",
    "    #train_df.drop(columns=['state_action_key'], inplace=True)\n",
    "    \n",
    "    # ******************************** # EXPERIMENTAL STEP END\n",
    "    \n",
    "    # check if the training dataset is empty \n",
    "    # (if empty, subsequent steps have to be skipped)\n",
    "    if not(train_df.shape[0]>0):\n",
    "        \n",
    "        # if training dataset is emtpy - return None (break the training loop)\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        ### computing action-preference counts for every action (for every states) ###\n",
    "        \n",
    "        # identify the 'prefered-action' at each 'state, action-pair' preference evaluation\n",
    "        train_df.loc[:,'prefered_action'] = train_df.apply(lambda row: row['a_j'][0][0] if row['preference_label'] == 1 else row['a_k'][0][0]  ,axis=1)\n",
    "\n",
    "        # compute the number of times each action is prefered at each state\n",
    "        action_preference_counts = train_df.groupby('state_key').prefered_action.value_counts().unstack()\n",
    "        action_preference_counts.replace(np.nan,0,inplace=True) # if an action is not preferred at a state, set pref. count to '0'\n",
    "\n",
    "        # remove the column index names of the `action_preference_counts' summary table\n",
    "        action_preference_counts.columns.name = None\n",
    "\n",
    "        # find any action(s) that was not preferred at all sampled states \n",
    "        # - this is important because a ranking for every possible action\n",
    "        #   at each state needs to be included in the training (label) data\n",
    "        missed_actions = [action for action in action_space if action not in action_preference_counts.columns.tolist()]\n",
    "        missed_actions = np.array(missed_actions).astype(action_preference_counts.columns.dtype) # convert to the same data-type of remaining columns\n",
    "\n",
    "        # add any missing actions to the `action_preference_counts' table\n",
    "        if len(missed_actions)>0:\n",
    "\n",
    "            # add the missing action (with a preference count of zero)\n",
    "            for action in missed_actions:\n",
    "                action_preference_counts.loc[:,action] = 0\n",
    "\n",
    "            # sort the actions in the summary according to arrangement in action space (ascending order)\n",
    "            action_preference_counts = action_preference_counts.reindex(sorted(action_preference_counts.columns), axis=1)    \n",
    "\n",
    "        \n",
    "        # convert the action-preference-counts (of actions at each state) to a vector and add it as a new column\n",
    "        #  - data in this column is used to create training labels\n",
    "        action_preference_counts.loc[:, 'preference_label_vector'] = pd.DataFrame({'label_data': action_preference_counts.iloc[:,0:].values.tolist()}).values\n",
    "\n",
    "        # append the column having action-preference-counts vectors to the training dataset\n",
    "        train_df = train_df.merge(right = action_preference_counts.loc[:,['preference_label_vector']]\n",
    "                                  , right_index= True\n",
    "                                  , left_on = 'state_key'\n",
    "                                  , how = 'left')\n",
    "        \n",
    "\n",
    "        # create the reduced training dataset \n",
    "        # - drop unnecessary columns & duplicate rows (which have duplicate data for same states)\n",
    "        train_df_reduced = train_df.loc[:,['state', 'state_key', 'preference_label_vector']]\n",
    "        train_df_reduced.drop_duplicates(subset=['state_key'],inplace=True)\n",
    "        train_df_reduced.preference_label_vector = train_df_reduced.preference_label_vector.apply(lambda row: np.array(row).astype(np.float)) # convert all label vectors to float\n",
    "        \n",
    "        if show_dataset:\n",
    "            print(f'\\nTraining data samples: {train_df_reduced.shape[0]}')\n",
    "            print(train_df_reduced.loc[:,['state_key', 'preference_label_vector']])\n",
    "        \n",
    "        ### preparing the training dataset for the neural network (LabelRanker) model ###\n",
    "\n",
    "        # normalize the action-preference-counts vectors (label data for the model)\n",
    "        # - this step produces the rankings:\n",
    "        # - i.e., the action(s) with the highest preference count(s) will have the highest value(s)\n",
    "        # - after normalization\n",
    "        output_labels_temp = np.array(train_df_reduced.preference_label_vector.tolist())\n",
    "        row_sums = output_labels_temp.sum(axis=1)\n",
    "        output_labels_normalized = output_labels_temp / row_sums[:, np.newaxis]\n",
    "        output_labels = torch.from_numpy(output_labels_normalized) # convert to tensor\n",
    "\n",
    "        # Generate the input state data tensors (feature data for the model)\n",
    "        #   This only includes 'pendulum-angle' and 'angular velocity'\n",
    "        #   State values are rounded; this seems to improve the performance\n",
    "        input_states  = torch.from_numpy(np.array(train_df_reduced.state.apply(lambda x: [round(x[2].reshape(-1)[0],5).astype(float), round(x[3].reshape(-1)[0],5).astype(float)]).tolist())) # only select pole-position and pole-velocity\n",
    "\n",
    "        \n",
    "        # create TensorDataset\n",
    "        train_ds = TensorDataset(input_states , output_labels)\n",
    "        \n",
    "        # define the batch size\n",
    "        batch_size = batch_s \n",
    "        \n",
    "        # define the data loader\n",
    "        train_dl = DataLoader(train_ds\n",
    "                              , batch_size\n",
    "                              , shuffle=True\n",
    "                              #, drop_last=True\n",
    "                             )\n",
    "        \n",
    "        \n",
    "    ### Defining and training the neural network (LabelRanker) model ###        \n",
    "    \n",
    "    class Model(nn.Module):\n",
    "\n",
    "        def __init__(self, input_state_len, output_label_len, layers, p=0.3):\n",
    "\n",
    "            super(Model,self).__init__()\n",
    "\n",
    "            all_layers = []\n",
    "            input_size = input_state_len\n",
    "\n",
    "            # create layers\n",
    "            for layer_dim in layers:\n",
    "                all_layers.append(nn.Linear(input_size, layer_dim))\n",
    "                all_layers.append(nn.LeakyReLU(inplace=True))\n",
    "                input_size = layer_dim\n",
    "\n",
    "            all_layers.append(nn.Linear(layers[-1], output_label_len))\n",
    "\n",
    "            self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "        def forward(self, state_vec):\n",
    "            x = self.layers(state_vec)\n",
    "            return x\n",
    "\n",
    "    # List to store losses\n",
    "    aggregated_losses = []\n",
    "\n",
    "    # Defining a function to train the model\n",
    "    def fit(num_epochs, model, loss_fn, opt):\n",
    "        \n",
    "        for _ in range(num_epochs):\n",
    "            for xb,yb in train_dl:\n",
    "\n",
    "                # Generate predictions\n",
    "                pred = model(xb.float())\n",
    "                loss = loss_fn(pred, yb.float())\n",
    "\n",
    "                # Perform gradient descent\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "\n",
    "            aggregated_losses.append(loss_fn(model(input_states.float()), output_labels.float()).detach().numpy())\n",
    "        \n",
    "        # return training loss\n",
    "        return loss_fn(model(input_states.float()), output_labels.float()).detach().numpy()\n",
    "    \n",
    "\n",
    "    # Path to save/load model\n",
    "    PATH = f_paths.paths['model_output'] + f\"{model_name}_pbpi_model.pt\"\n",
    "\n",
    "    if retrain_model:\n",
    "\n",
    "        if policy_iterr_count == 1:\n",
    "\n",
    "            if show_train_plot:\n",
    "                print(f'\\nModified-algo: Training iter:{policy_iterr_count}\\n')\n",
    "\n",
    "            # Create a NN model instance\n",
    "            model = Model(input_states.shape[1], output_labels.shape[1], mod_layers)\n",
    "\n",
    "            # Define optimizer and loss\n",
    "            opt = torch.optim.Adam(model.parameters(), lr = l_rate, weight_decay = 0.0001)\n",
    "            loss_fn = F.mse_loss\n",
    "\n",
    "            # Train the model\n",
    "            epochs = n_epochs\n",
    "            loss_v = fit(epochs, model, loss_fn, opt)\n",
    "\n",
    "            # Save the trained model\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "        \n",
    "        else:\n",
    "            if show_train_plot:\n",
    "                print(f'\\nModified-algo: Retraining iter:{policy_iterr_count}\\n')\n",
    "\n",
    "            # Create a NN model instance\n",
    "            model = Model(input_states.shape[1], output_labels.shape[1], mod_layers)\n",
    "\n",
    "            # Define optimizer and loss\n",
    "            decayed_lr =  (l_rate/(5*policy_iterr_count))\n",
    "            opt = torch.optim.Adam(model.parameters(), lr = decayed_lr, weight_decay = 0.0001)\n",
    "            loss_fn = F.mse_loss\n",
    "\n",
    "            # Load the model\n",
    "            model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "            # Train the model\n",
    "            epochs = n_epochs\n",
    "            loss_v = fit(epochs, model, loss_fn, opt)\n",
    "\n",
    "            # save the trained model\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Create a NN model instance\n",
    "        model = Model(input_states.shape[1], output_labels.shape[1], mod_layers)\n",
    "\n",
    "        # Define optimizer and loss\n",
    "        opt = torch.optim.Adam(model.parameters(), lr = l_rate, weight_decay = 0.0001)\n",
    "        loss_fn = F.mse_loss\n",
    "\n",
    "        # Train the model\n",
    "        epochs = n_epochs\n",
    "        loss_v = fit(epochs, model, loss_fn, opt)\n",
    "\n",
    "        # save the trained model\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    # plot the model loss\n",
    "    if show_train_plot:\n",
    "        plt.plot(range(epochs), aggregated_losses)\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.title(f'Training samples: {train_df_reduced.shape[0]} | Training loss: {str(np.round(loss_v,5))}\\n')\n",
    "        plt.show()\n",
    "\n",
    "    # set the model to evaluation mode and return it\n",
    "    return model.eval()\n",
    "\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ChemoSimulation-v0')\n",
    "\n",
    "# Dosages to provide to patient\n",
    "dosage_levels = np.array([1. ,.7 , .1, .7, 1., .7])\n",
    "\n",
    "# Create patient: initial Tumor size and wellness\n",
    "starting_state = init_patients[0][0], init_patients[0][1]\n",
    "\n",
    "# Simulate the environment\n",
    "env.reset(init_state = starting_state)\n",
    "\n",
    "state, reward, done, p_death = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2.0082598], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "state[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ChemoSimulation-v0')\n",
    "\n",
    "# Dosages to provide to patient\n",
    "dosage_levels = np.array([1. ,.7 , .1, .7, 1., .7])\n",
    "\n",
    "# Create patient: initial Tumor size and wellness\n",
    "starting_state = init_patients[0][0], init_patients[0][1]\n",
    "\n",
    "# Simulate the environment\n",
    "env.reset(init_state = starting_state)\n",
    "\n",
    "obs_list = []\n",
    "obs_list.append(starting_state)\n",
    "\n",
    "for dosage in dosage_levels:\n",
    "    obs, reward, done, p_death = env.step(np.array([dosage]))\n",
    "    obs_list.append(obs)\n",
    "    if done:\n",
    "        break"
   ]
  }
 ]
}